{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-omnibenchmark-documentation","title":"Welcome to Omnibenchmark documentation","text":"<p>Omnibenchmark is a benchmark project that aims to provide community-driven, modular, extensible and always up-to-date benchmarks.</p> <p>It is based on the Renku project, an open and collaborative data analysis platform.</p> <p>The framework connects data, methods and metrics repositories (a.k.a. modules), that can be flexibly extended by any community member.</p> <p>If you are interested in contributing a new method, dataset or metric, follow the documentation from the <code>Getting started</code> section, where you can learn more about how omnibenchmark works and how to extend it with a new module.</p> <p>If you are interested in exploring one of the existing benchmarks and the latest results, you can directly jump to the results of our evaluations in the <code>Output</code> section.</p> <p></p>"},{"location":"01_landing/","title":"Motivation","text":"<p>Benchmarking is a critical step for the development of bioinformatic methods and provides important insights for their application.</p> <p>The current benchmarking scheme has many limitations:</p> <ul> <li> <p>It is a snapshot of the available methods at a certain time point and often already outdated when published.</p> </li> <li> <p>Comparison of benchmarks is challenging: different procedures, different datasets, different evaluation criteria, etc.</p> </li> <li> <p>All of the above can lead to different conclusions among benchmarks made at different time points or at different groups.</p> </li> </ul>"},{"location":"01_landing/#concept","title":"Concept","text":"<p>Omnibenchmark is a modular and extensible framework based on a free open-source analytic platform, Renku, to offer a continuous and open community benchmarking system.</p> <p>The framework consists of data, method and metric repositories (or \u201cmodules\u201d) that share data components and are tracked via a knowledge graph from the Renku system. The results can be displayed in an interactive dashboard to be openly explored by anyone looking for recommendations of tools. New data, methods or metrics can be added by the community to extend the benchmark.</p> <p>Some key features of our benchmarking framework:</p> <ul> <li> <p>Periodical updates of the benchmark to provide up-to-date results</p> </li> <li> <p>Easy extensibility through templates for data, methods or metrics</p> </li> <li> <p>Following FAIR principles by using software containers, an integration with Gitlab and full provenance tracking (inputs, workflows and generated files)</p> </li> <li> <p>Flexibility to work with different benchmarking structures, topics and programming languages.</p> </li> </ul>"},{"location":"01_landing/#prototypes","title":"Prototypes","text":"<p>We are currently building multiple prototypes for community-based benchmarking of single cell batch correction methods. </p> <p>The list of current benchmarks and their results can be explored on the official Omnibenchmark website: </p> <p>http://omnibenchmark.org/</p>"},{"location":"about/","title":"About","text":""},{"location":"about/#omnibenchmark-team","title":"Omnibenchmark team","text":"<p>Omnibenchmark is being developed by members of the Mark D. Robinson lab.</p> <p>We are located in the Department of Molecular Life Sciences within the Faculty of Science at the University of Zurich.</p> <p>Research in our laboratory is focused on creating, comparing and understanding statistical methods and data science tools for processing and interpreting various types of genomic data. </p> <p>Contact.</p>"},{"location":"01_getting_started/","title":"Getting started","text":"<p>Depending on your role, navigate to the corresponding section </p> <ul> <li> <p>Module contributor (basic users): you would like to submit a new dataset, method or metric ? The next section will explain you how to create and submit a new module to an Omnibenchmark. </p> </li> <li> <p>Benchmarker (advanced users): you would like to create a whole new Omnibenchmark ? The second section will show you how to configure and populate a benchmark on our system. </p> </li> <li> <p>Method user (analysts): you would like to access the latest results of an Omnibenchmark ? The third section will redirect you to shiny apps that will allow you to explore the results of our omnibenchmarks. </p> </li> </ul>"},{"location":"01_getting_started/01_module_contr/","title":"Module contributor","text":"<p>This section is dedicated for module contributors, basic users that would like to add a new module (dataset, method, metric) to an existing Omnibenchmark. This section will show how to add a new module to an existing Omnibenchmark. </p> <p>This section will not show you how to create/ setup an Omnibenchmark (see the <code>Benchmarker</code> section) and how to visualize and interpret the evaluation results (see the <code>Method user</code> section).</p> <p>Warning</p> <p>Before starting, make sure that you have;</p> <ol> <li> <p>Created an account to access Renku;</p> </li> <li> <p>Requested an access to the Omnibenchmark Gitlab group (<code>Request Access</code> button. If you don't see it, you were already granted access to it);</p> </li> <li> <p>A script for importing data, or a wrapper of a method or a metric.</p> </li> </ol>"},{"location":"01_getting_started/01_module_contr/create_module/","title":"1. Create a module","text":"<p>Any part of Omnibenchmark can be extended with an additional project that will add a new dataset, method or metric to the evaluation. </p> <p>To ease the addition of a new project to Omnibenchmark, you can select a template when creating a new Renku project. </p> <p>Info</p> <p>A template is a project that was already pre-filled and configured to work with Omnibenchmark in a way that you won't have to know too much about Renku. You will just have to add your data and/or your code to project that will extend Omnibenchmark. </p> <p>The Scribe guide below will show you how to create a new Omnibenchmark project for any module (data, method, metric,...).</p> <p>If your benchmark is already in the list below, you can click on the module that you would like to add and jump to step 11 of the Scribe guide; </p> <p>Batch-correction Omnibenchmark </p> <ul> <li> <p>Data</p> </li> <li> <p>Method</p> </li> <li> <p>Metric</p> </li> </ul> <p>Clustering Omnibenchmark: </p> <ul> <li> <p>Data</p> </li> <li> <p>Method</p> </li> <li> <p>Metric</p> </li> </ul> <p>Spatial clustering Omnibenchmark: </p> <ul> <li> <p>Data</p> </li> <li> <p>Method</p> </li> <li> <p>Metric</p> </li> </ul>"},{"location":"01_getting_started/01_module_contr/setup_module/","title":"2. Set up a module","text":"<p>The basic steps required to set up a module are described in the README of your project. Depending on your project, you may also have to modify the following files of your project: </p> <ul> <li> <p><code>Dockerfile</code>; extra linux (ubuntu) software requirements can be specified here (<code>R</code> and <code>Python</code> installed by default).</p> </li> <li> <p><code>install.R</code>; to manage extra R dependencies</p> </li> <li> <p><code>requirements.txt</code>; to manage extra Python dependencies</p> </li> </ul> <p>Detailed instruction can be found in the Reku documentation.</p> <p>Upon any commit, Renku will automatically build a new Docker container based on these files. </p> <p>Info</p> <p>Having independent modules also means that you can test your code and work on your project without affecting the omnibenchmark until you submit it. Only when a project is included into an omnibenchmark 'orchestrator', it becomes part of the benchmark itself.</p>"},{"location":"01_getting_started/01_module_contr/setup_module/#detailed-instructions-for","title":"Detailed instructions for","text":"<ul> <li> <p>Input data; Dataset with known ground-truth to run methods on.</p> </li> <li> <p>Methods; Methods that will be applied on the (processed) datasets and have their output evaluated. Also use their </p> </li> <li> <p>Metrics; Performance metrics used to evaluate methods outputs. </p> </li> </ul>"},{"location":"01_getting_started/01_module_contr/setup_module/01_data/","title":"Data modules","text":"<p>Data modules are modules that define input datasets and bundle them into a renku dataset, that can be imported by other projects. This page goes into details of some important elements to configure data. Most modules contain 3 main files:</p>"},{"location":"01_getting_started/01_module_contr/setup_module/01_data/#1-the-configyaml-file","title":"1. The config.yaml file","text":"<p>The <code>config.yaml</code> (in your <code>src</code> directory) defines the metadata associated to your dataset and how it will be run. Upon creation of a dataset project, a <code>src/config.yaml</code> is already created and some metadata already pre-filled. Let's have a look how you can configure it; </p> <pre><code>---\ndata:\nname: \"dataset-name\"\ntitle: \"dataset title\"\ndescription: \"A new dataset module for omnibenchmark\"\nkeywords: [\"MODULE_KEY\"]\nscript: \"path/to/module_script\"\noutputs:\nfiles:\ndata_file1: end: \"FILE1_END\"\ndata_file2:\nend: \"FILE2_END\"\ndata_file3:\nend: \"FILE3_END\"\nbenchmark_name: \"OMNIBENCHMARK_TYPE\"\n</code></pre> <ul> <li><code>data: name:</code> (mandatory) Specifies the dataset name. Do not use special characters outside <code>-</code> and <code>_</code>. </li> </ul> <p>Warning</p> <p>Make the name as explicit as possible. If it (even partially) overlaps with other projects names, it might create some conflicts. For example, prefer <code>iris_dataset_clustering</code> instead of <code>iris</code>. </p> <ul> <li> <p><code>data: keywords:</code> (mandatory) A keyword that is used for all datasets of this omnibenchmark. By default/ convention: <code>[omnibenchmark name]_data</code>. If you are not sure, you can check the keywords already in use in existing omnibenchmarks. </p> </li> <li> <p><code>script:</code> (mandatory) Relative path to the script to execute. More information about the script in the <code>The module script</code> section below. </p> </li> <li> <p><code>outputs:files:</code> (Mandatory) Name that will be passed to the output of the scripts (e.g. <code>data_file1</code>) and their file extension (<code>end:</code> field). It must match the arguments of you script (see <code>The module script</code> section below).  </p> </li> <li> <p><code>benchmark_name:</code> (Mandatory) The name of the omnibenchmark this dataset belongs to. If you are not sure, you can check the names already in use in existing omnibenchmarks. </p> </li> <li> <p><code>data: title:</code> (Optional) A human readable title for this dataset. Can be the same as <code>data: name:</code>. </p> </li> <li> <p><code>data: description:</code> (Optional) Some description about this dataset. </p> </li> </ul> <p>More information about the <code>config.yaml</code> file in the Setup a config.yaml section.</p>"},{"location":"01_getting_started/01_module_contr/setup_module/01_data/#2-the-module-script","title":"2. The module script","text":"<p>This is the script to load the dataset and to convert its files into the expected format. Omnibenchmark accepts any kind of script and its maintenance and content is up to the module author. By default, an R and Python script are provided in the <code>src</code> folder that you can use to import or create your dataset. Omnibenchmark calls the script (specified in the <code>script:</code> field of <code>config.yaml</code>) from the command line. If you use another language than R or Python (e.g. Julia or bash) specify the interpreter to use in the corresponding field of the <code>config.yaml</code> file (<code>script:</code> field of <code>config.yaml</code>).</p> <p>Note</p> <p>All input and output files will automatically be parsed from the command line in the format: <code>--ARGUMENT_NAME ARGUMENT_VALUE</code></p> <p>You can find below a dummy example of how a <code>config.yaml</code> and its corresponding script can look like (in Python and R); </p> config.yaml <pre><code>---\ndata:\nname: \"some-dataset\"\ntitle: \"some dataset\"\ndescription: \" \"\nkeywords: [\"omnibenchmark-X_data\"] script: \"src/dataset.R\" # (1)!\noutputs:\ntemplate: \"data/${name}/${name}_${out_name}.${out_end}\"\nfiles:\ndataset_output: # (2)!\nend: \"csv\" # (3)!\nbenchmark_name: \"omnibenchmark-X\" </code></pre> <ol> <li> <p>Can be replaced by another script or extension. </p> </li> <li> <p>The argument name that will be parsed when running the script. Has to match the argument name of your script! (showed in the next script)</p> </li> <li> <p>Specifies the file extension (attached to the argument name). </p> </li> </ol> <p>The corresponding script that creates a <code>dataset_output.csv</code> (following our example <code>config.yaml</code>) can thus look like this; </p> dataset.pydataset.R <pre><code># Load module\nimport argparse\n# Get command line arguments and store them in args\nparser=argparse.ArgumentParser()\nparser.add_argument('--dataset_output', help='name') # (1)!\nargs=parser.parse_args()\n# Call the argument\narg1 = args.argument_name \n# Import or create a dataset\n# df=...\n# Save as csv using parsed argument\ndf.to_csv(arg1, index=False) # (2)!\n</code></pre> <ol> <li> <p>Argument name should correspond to the one in config, here 'dataset_output'. </p> </li> <li> <p>No need to specify the file extension here. For outputs, it will automatically be parsed based on the 'end:' that you specified in your config.yaml. </p> </li> </ol> <pre><code># Load package\nlibrary(optparse)\n# Get list with command line arguments by name\noption_list = list(\nmake_option(c(\"--dataset_output\"), # (1)!\ntype=\"character\", default=NULL, help=\" \", metavar=\"character\")\n); opt_parser = OptionParser(option_list=option_list);\nopt = parse_args(opt_parser);\n# Call the argument\narg1 &lt;- opt$argument_name\n# Import of create a dataset \n# df &lt;- ...\nwrite.csv(df, file = arg1) # (2)!\n</code></pre> <ol> <li> <p>Argument name should correspond to the one in config, here 'dataset_output'. </p> </li> <li> <p>No need to specify the file extension here. For outputs, it will automatically be parsed based on the 'end:' that you specified in your config.yaml. </p> </li> </ol> <p>For multiple outputs, simply add more fields to the <code>outputs: files:</code> of your <code>config.yaml</code> and the same names to argument parser of your script. </p> Special case: input data as files <p>In some cases the data have to be uploaded manually to the project instead of being fetched/ downloaded by a script. (Please note however that downloading the data from a script is more reproducible and allows to keep the data, and thus your benchmark, always up-to-date!)</p> <p>For example, say that we have uploaded 2 data files in an <code>input</code> folder. The corresponding <code>config.yaml</code> and R/Python script would be; </p> config.yaml <pre><code>---\ndata:\nname: \"some-dataset\"\ntitle: \"some dataset\"\ndescription: \" \"\nkeywords: [\"omnibenchmark-X_data\"] script: \"src/dataset.R\" inputs: # (1)!\ninput_files: import_this_dataset:\ninput_data: \"input/input_dataset.txt\" # (2)!\ninput_metadata: \"input/input_metadata.txt\"\noutputs:\ntemplate: \"data/${name}/${name}_${out_name}.${out_end}\"\nfiles:\ndataset_output:\nend: \"csv\" benchmark_name: \"omnibenchmark-X\" </code></pre> <ol> <li> <p>Section to add to your config.yaml</p> </li> <li> <p>Two new arguments, 'input_data' and 'input_metadata' will now be sent to your script, containing the path to your input files. </p> </li> </ol> <p>And the corresponding script; </p> dataset.pydataset.R <pre><code># Load module\nimport argparse\n# Get command line arguments and store them in args\nparser=argparse.ArgumentParser()\nparser.add_argument('--dataset_output', help='name')\nparser.add_argument('--input_data', help='name') # (1)!\nparser.add_argument('--input_metadata', help='name') \nargs=parser.parse_args()\n# Call the argument\narg1 = args.argument_name \n# Import or create a dataset\n# df=...\n# Save as csv using parsed argument\ndf.to_csv(arg1, index=False) \n</code></pre> <ol> <li>Argument names that correspond to the 2 'input:' fields of the config.yaml. </li> </ol> <pre><code># Load package\nlibrary(optparse)\n# Get list with command line arguments by name\noption_list = list(\nmake_option(c(\"--dataset_output\"),\ntype=\"character\", default=NULL, help=\" \", metavar=\"character\"),\nmake_option(c(\"--input_data\"), # (1)!\ntype=\"character\", default=NULL, help=\" \", metavar=\"character\"), make_option(c(\"--input_metadata\"),\ntype=\"character\", default=NULL, help=\" \", metavar=\"character\")\n); opt_parser = OptionParser(option_list=option_list);\nopt = parse_args(opt_parser);\n# Call the argument\narg1 &lt;- opt$argument_name\n# Import of create a dataset \n# df &lt;- ...\nwrite.csv(df, file = arg1) </code></pre> <ol> <li>Argument name should correspond to the one in config, here 'dataset_output'. </li> </ol>"},{"location":"01_getting_started/01_module_contr/setup_module/01_data/#3-the-run_workflowpy-file","title":"3. The run_workflow.py file","text":"<p>This file is to generate, run and update the modules dataset and workflow. This is a standard script that will adapt to your <code>config.yaml</code> and dataset script and does not need to be (and should not be!) modified. Below is a copy of the <code>run_workflow.py</code> that you have in your project, with some explanations: </p> run_workflow.py <pre><code># Load modules\nfrom omnibenchmark.utils.build_omni_object import get_omni_object_from_yaml\nfrom omnibenchmark.renku_commands.general import renku_save\n# Build an OmniObject from the config.yaml file\nomni_obj = get_omni_object_from_yaml('src/config.yaml') # (1)!\n# Create the output dataset\nomni_obj.create_dataset() # (2)!\nrenku_save()\n## Run and update the workflow\nomni_obj.run_renku()  # (3)!\nrenku_save()\n## Add files to output dataset\nomni_obj.update_result_dataset() # (4)!\nrenku_save()\n</code></pre> <ol> <li> <p>Load the options of your dataset into an Omnibenchmark object. </p> </li> <li> <p>Pay attention to any warning messages about names conflicts (it can lead to unpredictable conflicts latter in the benchmark). If it happens, change the 'name' in your config.yaml and rerun all previous command lines. </p> </li> <li> <p>This will run your script with parsed arguments. </p> </li> <li> <p>This will upload the created dataset to make it recognizable by further modules. </p> </li> </ol> <p>Once your module is completed and tested, you can submit it to the team. </p>"},{"location":"01_getting_started/01_module_contr/setup_module/02_method/","title":"Method modules","text":"<p>A method module imports all datasets of a benchmark or all preprocessed inputs and runs one benchmarking method on them. Method outputs are added to a renku dataset, that can be imported by metric projects to evaluate them. Benchmark specific requirements like file formats, types and prefixes can be checked at the omnibenchmark website. </p> <p>To test a method, a parameter dataset is imported to test the method on different parameter settings. Usually the parameter space is defined Valid parameters can be specified in the config.yaml file or on a filtered set of parameters (e.g. parameter limits, specific values and parameter and input file combinations). Usually a method module contains only one workflow, that is automatically run with all valid parameter and input file combinations. Most modules contain 3 main files:</p>"},{"location":"01_getting_started/01_module_contr/setup_module/02_method/#1-the-configyaml-file","title":"1. The config.yaml file","text":"<p>The <code>config.yaml</code> (in your <code>src</code> directory) defines the metadata associated to your method and how it will be tested. Upon creation of a method project, a <code>src/config.yaml</code> is already created and some metadata already pre-filled. Let's have a look how you can configure it; </p> config.yaml <pre><code>---\ndata:\nname: \"method name\" # (1)! \ntitle: \"method title\"\ndescription: \"Short description of the method\"\nkeywords: [\"MODULE_KEY\"] # (2)!\nscript: \"path/to/module_script\" # (3)!\nbenchmark_name: \"OMNIBENCHMARK_TYPE\" # (4)!\ninputs:\nkeywords: [\"INPUT_KEY\"] # (5)!\nfiles: [\"input_file_name1\", \"input_file_name2\"]\nprefix:\ninput_file_name1: \"_INPUT1_\"\ninput_file_name2: \"_INPUT2_\"\noutputs:\nfiles:\nmethod_result1: # (6)!\nend: \"FILE1_END\"\nmethod_result2:\nend: \"FILE2_END\"\nparameter:\nkeywords: [\"PARAMETER_KEY\"]# (7)!\nnames: [\"parameter1\", \"parameter2\"] </code></pre> <ol> <li> <p>Specifies the method name. Do not use special characters outside <code>-</code> and <code>_</code>. </p> </li> <li> <p>A keyword that is used for all methods of this omnibenchmark. By default/ convention: <code>[omnibenchmark name]_method</code>. If you are not sure, you can check the keywords already in use in existing omnibenchmarks. </p> </li> <li> <p>Relative path to the script to execute. More information about the script in the <code>The module script</code> section below. </p> </li> <li> <p>The name of the omnibenchmark this method belongs to. If you are not sure, you can check the names already in use in existing omnibenchmarks. </p> </li> <li> <p>The keyword(s) of the input dataset to test the method on. If you are not sure, you can check the keywords already in use in existing omnibenchmarks. </p> </li> <li> <p>Name that will be passed to the output of the scripts (e.g. <code>method_result1</code>) and their file extension (<code>end:</code> field). It must match the arguments of you script (see <code>The module script</code> section below). </p> </li> <li> <p>The keyword of the input parameters to test the method on and their names. If you are not sure, you can check the keywords already in use in existing omnibenchmarks. You can also specify define specific parameters, see the dedicated documentation of the <code>config.yaml</code> file.</p> </li> </ol> <p>More information about the <code>config.yaml</code> file in the Setup a config.yaml section,</p>"},{"location":"01_getting_started/01_module_contr/setup_module/02_method/#2-the-module-script","title":"2. The module script","text":"<p>This is the script to apply a metric on test datasets. Omnibenchmark accepts any kind of script and its maintenance and content is up to the module author. By default, an R and Python script are provided in the <code>src</code> folder that you can use to add your method's wrapper. Omnibenchmark calls this script from the command line. If you use another language than R or Python (e.g. Julia or bash) specify the interpreter to use in the corresponding field of the <code>config.yaml</code> file (<code>script:</code> field of <code>config.yaml</code>).</p> <p>In Python argparse can be used to parse command arguments. In R we recommend to use the optparse package. A basic example is provided in the two example scripts provided in your project. </p> <p>Note</p> <p>All input and output files will automatically be parsed from the command line in the format: <code>--ARGUMENT_NAME ARGUMENT_VALUE</code></p> <p>Below is a dummy example of how the R/Python scripts can look like given the example <code>config.yaml</code> from the previous section;  </p> method.pymethod.R <pre><code># Load module\nimport argparse\n# Get command line arguments and store them in args\nparser=argparse.ArgumentParser()\nparser.add_argument('--input_file_name1', help='name') # (1)!\nparser.add_argument('--input_file_name2', help='name') \nparser.add_argument('--method_result1', help='name') \nparser.add_argument('--method_result2', help='name') \nparser.add_argument('--parameter1', help='name') \nparser.add_argument('--parameter2', help='name') \nargs=parser.parse_args()\n# Call the arguments\ninput_file_name1 = args.input_file_name1 \ninput_file_name2 = args.input_file_name2 \nmethod_result1 = args.method_result1 \nmethod_result2 = args.method_result2 \nparameter1 = args.parameter1 \nparameter2 = args.parameter2 \n# Run the method to be benchmarked and create 2 outputs\n# output1=...\n# output2=...\n# Save as csv using parsed argument\noutput1.to_csv(method_result1, index=False) # (2)!\noutput2.to_csv(method_result2, index=False) \n</code></pre> <ol> <li> <p>Argument names that correspond to the  <code>inputs</code>, <code>outputs</code> and <code>parameter</code> fields specified in <code>config.yaml</code>. </p> </li> <li> <p>No need to specify the file extension here. For outputs, it will automatically be parsed based on the 'end:' that you specified in your config.yaml. </p> </li> </ol> <pre><code># Load package\nlibrary(optparse)\n# Get list with command line arguments by name\noption_list = list(\nmake_option(c(\"--input_file_name1\"), # (1)!\ntype=\"character\", default=NULL, help=\" \"), make_option(c(\"--input_file_name2\"),\ntype=\"character\", default=NULL, help=\" \"), make_option(c(\"--method_result1\"),\ntype=\"character\", default=NULL, help=\" \"), make_option(c(\"--method_result2\"),\ntype=\"character\", default=NULL, help=\" \"), make_option(c(\"--parameter1\"),\ntype=\"character\", default=NULL, help=\" \"), make_option(c(\"--parameter2\"),\ntype=\"character\", default=NULL, help=\" \"), ); opt_parser = OptionParser(option_list=option_list);\nopt = parse_args(opt_parser);\n# Call the argument\ninput_file_name1 &lt;- opt$input_file_name1\ninput_file_name2 &lt;- opt$input_file_name2\nmethod_result1 &lt;- opt$method_result1\nmethod_result2 &lt;- opt$method_result2\nparameter1 &lt;- opt$parameter1\nparameter2 &lt;- opt$parameter2\n# Run the method to be benchmarked and create 2 outputs\n# output1 &lt;- ...\n# output2 &lt;- ...\n# Save as csv using parsed argument\nwrite.csv(output1, file = method_result1) # (2)!\nwrite.csv(output2, file = method_result2) </code></pre> <ol> <li> <p>Argument names that correspond to the  <code>inputs</code>, <code>outputs</code> and <code>parameter</code> fields specified in <code>config.yaml</code>.</p> </li> <li> <p>No need to specify the file extension here. For outputs, it will automatically be parsed based on the 'end:' that you specified in your config.yaml. </p> </li> </ol>"},{"location":"01_getting_started/01_module_contr/setup_module/02_method/#3-the-run_workflowpy-file","title":"3. The run_workflow.py file","text":"<p>This file is to run and update the method workflow. This is a standard script that will adapt to your <code>config.yaml</code> and dataset script and does not need to be (and should not be!) modified. Below is a copy of the <code>run_workflow.py</code> that you have in your project, with some explanations: </p> run_workflow.py <pre><code>from omnibenchmark.utils.build_omni_object import get_omni_object_from_yaml\nfrom omnibenchmark.renku_commands.general import renku_save\nimport omniValidator as ov\nrenku_save()\n## Load config\nomni_obj = get_omni_object_from_yaml('src/config.yaml')  # (1)!\n## Update object and download input datasets\nomni_obj.update_object(all=True) # (2)! \nrenku_save()\n## Check object\nprint(\nf\"Object attributes: \\n {omni_obj.__dict__}\\n\"\n)\nprint(\nf\"File mapping: \\n {omni_obj.outputs.file_mapping}\\n\"\n)\nprint(\nf\"Command line: \\n {omni_obj.command.command_line}\\n\" # (3)!\n)\n## Create output dataset\nomni_obj.create_dataset() # (4)!\n## Run workflow\nov.validate_requirements(omni_obj)\nomni_obj.run_renku(all=False) # (5)!\nrenku_save()\n## Update Output dataset\nomni_obj.update_result_dataset() # (6)!\nrenku_save()\n</code></pre> <ol> <li> <p>Load the options of your dataset into an Omnibenchmark object. </p> </li> <li> <p>Downloads the datasets specified in <code>inputs:</code>. It case there are too many data to download, <code>all</code> can be set to False to only download a subset of the input data. </p> </li> <li> <p>This will show the command line and the parsed arguments that are going to be called to run the method. Can also be used for debugging. </p> </li> <li> <p>Pay attention to any warning messages about names conflicts (it can lead to unpredictable conflicts latter in the benchmark). If it happens, change the 'name' in your config.yaml and rerun all previous command lines.</p> </li> <li> <p>This will run your script with parsed arguments. </p> </li> <li> <p>This will upload the methods results to make it recognizable by further modules. </p> </li> </ol> <p>Once your module is completed and tested, you can submit it to the team. </p>"},{"location":"01_getting_started/01_module_contr/setup_module/03_metric/","title":"Metric modules","text":"<p>A metric module imports method result datasets and runs one evaluation on them. Evaluation results are added to a renku dataset, that can be summarized and explored in an omnibenchmark dashboard.  Benchmark specific requirements like file formats, types and prefixes can be checked at the omnibenchmark website. Usually a metric module contains two workflows, one to evaluate the results and one to generate the <code>metric info file</code>. Most metric modules contain 5 main files:</p>"},{"location":"01_getting_started/01_module_contr/setup_module/03_metric/#1-the-configyaml-file","title":"1. The config.yaml file","text":"<p>The <code>config.yaml</code> (in your <code>src</code> directory) defines the metadata associated to your method and how it will be tested. Upon creation of a method project, a <code>src/config.yaml</code> is already created and some metadata already pre-filled. Let's have a look how you can configure it; </p> config.yaml <pre><code>---\ndata:\nname: \"metric name\" # (1)! \ntitle: \"metric title\"\ndescription: \"Short description of the metric\"\nkeywords: [\"MODULE_KEY\"] # (2)!\nscript: \"path/to/module_script\" # (3)!\nbenchmark_name: \"OMNIBENCHMARK_TYPE\" # (4)!\ninputs:\nkeywords: [\"INPUT_KEY\"] # (5)!\nfiles: [\"input_file_name1\", \"input_file_name2\"]\nprefix:\ninput_file_name1: \"_INPUT1_\"\ninput_file_name2: \"_INPUT2_\"\noutputs:\nfiles:\nmetric_result: # (6)!\nend: \"FILE1_END\"\n</code></pre> <ol> <li> <p>Specifies the method name. Do not use special characters outside <code>-</code> and <code>_</code>. </p> </li> <li> <p>A keyword that is used for all metrics of this omnibenchmark. By default/ convention: <code>[omnibenchmark name]_metric_</code>. If you are not sure, you can check the keywords already in use in existing omnibenchmarks. </p> </li> <li> <p>Relative path to the script to execute. More information about the script in the <code>The module script</code> section below. </p> </li> <li> <p>The name of the omnibenchmark this method belongs to. If you are not sure, you can check the names already in use in existing omnibenchmarks. </p> </li> <li> <p>The keyword(s) of the methods results to evaluate. If you are not sure, you can check the keywords already in use in existing omnibenchmarks.</p> </li> <li> <p>Name that will be passed to the output of the scripts (e.g. <code>metric_result</code>) and their file extension (<code>end:</code> field). It must match the arguments of you script (see <code>The module script</code> section below).</p> </li> </ol> <p>More information about the <code>config.yaml</code> file in the Setup a config.yaml section.</p>"},{"location":"01_getting_started/01_module_contr/setup_module/03_metric/#2-the-module-script","title":"2. The module script","text":"<p>This is the script to load the method results and evaluate them using metrics. Omnibenchmark accepts any kind of script and its maintenance and content is up to the module author. By default, an R and Python script are provided in the <code>src</code> folder that you can use to add your method's wrapper. Omnibenchmark calls this script from the command line. If you use another language than R or Python (e.g. Julia or bash) specify the interpreter to use in the corresponding field of the <code>config.yaml</code> file (<code>script:</code> field of <code>config.yaml</code>).</p> <p>In Python argparse can be used to parse command arguments. In R we recommend to use the optparse package. A basic example is provided in the two example scripts provided in your project. </p> <p>Note</p> <p>All input and output files will automatically be parsed from the command line in the format: <code>--ARGUMENT_NAME ARGUMENT_VALUE</code></p> <p>Below is a dummy example of how the R/Python scripts can look like given the example <code>config.yaml</code> from the previous section;  </p> metric.pymetric.R <pre><code># Load module\nimport argparse\n# Get command line arguments and store them in args\nparser=argparse.ArgumentParser()\nparser.add_argument('--input_file_name1', help='Description of the argument') # (1)!\nparser.add_argument('--input_file_name2', help='Description of the argument')\nparser.add_argument('--metric_result', help='Description of the argument')\nargs=parser.parse_args()\n# Call the argument\ninput_file_name1 = args.input_file_name1\ninput_file_name2 = args.input_file_name2\nmetric_result = args.metric_result\n# Run the metric to benchmark the methods results\n# output=...\n# Save as csv using parsed argument\noutput.to_csv(metric_result, index=False) # (2)!\n</code></pre> <ol> <li> <p>Argument names that correspond to the  <code>inputs</code> and <code>outputs</code> fields specified in <code>config.yaml</code>. </p> </li> <li> <p>No need to specify the file extension here. For outputs, it will automatically be parsed based on the 'end:' that you specified in your config.yaml. </p> </li> </ol> <pre><code># Load package\nlibrary(optparse)\n# Get list with command line arguments by name\noption_list = list(\nmake_option(c(\"--input_file_name1\"), # (1)!\ntype=\"character\", default=NULL, help=\" \"), make_option(c(\"--input_file_name2\"),\ntype=\"character\", default=NULL, help=\" \"), make_option(c(\"--method_result\"),\ntype=\"character\", default=NULL, help=\" \")\n); opt_parser = OptionParser(option_list=option_list);\nopt = parse_args(opt_parser);\n# Call the argument\ninput_file_name1 &lt;- opt$input_file_name1\ninput_file_name2 &lt;- opt$input_file_name2\nmethod_result &lt;- opt$method_result\n# Run the method to be benchmarked and create 2 outputs\n# output &lt;- ...\n# Save as csv using parsed argument\nwrite.csv(output, file = metric_result) # (2)!\n</code></pre> <ol> <li> <p>Argument names that correspond to the  <code>inputs</code> and <code>outputs</code> fields specified in <code>config.yaml</code>.</p> </li> <li> <p>No need to specify the file extension here. For outputs, it will automatically be parsed based on the 'end:' that you specified in your config.yaml. </p> </li> </ol>"},{"location":"01_getting_started/01_module_contr/setup_module/03_metric/#3-the-generate_metric_infopy-file-optional","title":"3. The generate_metric_info.py file (optional)","text":"<p>This file could also be written in R or any other language but one is automatically provided in your <code>src</code> folder. You can change the metadata of the metric (indicated by a <code>+</code>), which will be used for its display into the dashboard showing the metric results. All options can also be left as default. </p> generate_metric_info.py <pre><code>import argparse\nimport json\nparser=argparse.ArgumentParser()\nparser.add_argument('--metric_info', help='Path to the metric info json file')\nargs=parser.parse_args()\nmetric_info = {\n'flip': False, # (1)!\n'max': 1, # (2)!\n'min': 0, # (3)!\n'group': \"METRIC_GROUP\", # (4)!\n'name': \"metric_name\", # (5)!\n'input': \"metric_input_type\" # (6)!\n}\nwith open(args.metric_info, \"w\") as fp:\njson.dump(metric_info , fp, indent=3) \n</code></pre> <ol> <li> <p>Logical scalar; whether or not to flip the sign of the metric values on the dashboard app. </p> </li> <li> <p>Possible maximum value of the metric. </p> </li> <li> <p>Possible minimum value of the metric. </p> </li> <li> <p>If different groups of metrics are added, a name can be given for grouping. Can also be left as 'default'. </p> </li> <li> <p>Metric name that will be displayed on the dashboard. </p> </li> <li> <p>Prefix to name the result files. Can be left as default; 'metric_res'. </p> </li> </ol>"},{"location":"01_getting_started/01_module_contr/setup_module/03_metric/#4-the-info_configyaml-file-optional","title":"4. The info_config.yaml file (optional)","text":"<p>This file contains file to complement the <code>generate_metric_info.py</code>. All fields can be left as default. </p>"},{"location":"01_getting_started/01_module_contr/setup_module/03_metric/#5-the-run_workflowpy-file","title":"5. The run_workflow.py file","text":"<p>This file is to run and update the metric project. This is a standard script that will adapt to your <code>config.yaml</code> and dataset script and does not need to be (and should not be!) modified. Below is a copy of the <code>run_workflow.py</code> that you have in your project, with some explanations: </p> run_workflow.py <pre><code>from omnibenchmark.utils.build_omni_object import get_omni_object_from_yaml\nfrom omnibenchmark.renku_commands.general import renku_save\nimport omniValidator as ov\nrenku_save()\n## Load config\nomni_obj = get_omni_object_from_yaml('src/config.yaml')  # (1)!\n## Update object and download input datasets\nomni_obj.update_object(all=True)  # (2)!\nrenku_save()\n## Check object\nprint(\nf\"Object attributes: \\n {omni_obj.__dict__}\\n\"\n)\nprint(\nf\"File mapping: \\n {omni_obj.outputs.file_mapping}\\n\"\n)\nprint(\nf\"Command line: \\n {omni_obj.command.command_line}\\n\"  # (3)!\n)\n## Create output dataset\nomni_obj.create_dataset() # (4)!\n## Run workflow\nov.validate_requirements(omni_obj)\nomni_obj.run_renku(all=False)  # (5)!\nrenku_save()\n## Update Output dataset\nomni_obj.update_result_dataset() # (6)!\nrenku_save()\n###################### Generate info file ###################### # (7)!\nomni_info = get_omni_object_from_yaml('src/info_config.yaml') \nomni_info.wflow_name = \"{{ sanitized_project_name }}_info\"\n## Check object\nprint(\nf\"Object attributes: \\n {omni_info.__dict__}\\n\"\n)\nprint(\nf\"File mapping: \\n {omni_info.outputs.file_mapping}\\n\"\n)\nprint(\nf\"Command line: \\n {omni_info.command.command_line}\\n\"\n)\n## Run workflow\nomni_info.run_renku()\nrenku_save()\n## Update Output dataset\nomni_info.update_result_dataset()\nrenku_save()\n################################################################\n</code></pre> <ol> <li> <p>Load the options of your dataset into an Omnibenchmark object. </p> </li> <li> <p>Downloads the datasets specified in <code>inputs:</code>. It case there are too many data to download, <code>all</code> can be set to False to only download a subset of the input data. </p> </li> <li> <p>This will show the command line and the parsed arguments that are going to be called to run the method. Can also be used for debugging. </p> </li> <li> <p>Pay attention to any warning messages about names conflicts (it can lead to unpredictable conflicts latter in the benchmark). If it happens, change the 'name' in your config.yaml and rerun all previous command lines.</p> </li> <li> <p>This will run your script with parsed arguments. </p> </li> <li> <p>This will upload the methods results to make it recognizable by further modules. </p> </li> <li> <p>Similar as before, but for the metrics metadata. </p> </li> </ol> <p>Once your module is completed and tested, you can submit it to the team. </p>"},{"location":"01_getting_started/01_module_contr/setup_module/04_submit/","title":"Submit a module to an Omnibenchmark","text":"<p>An omnibenchmark module will be able to import datasets from other modules and export its output to others. However, it still needs to be integrated in an Omnibenchmark orchestrator. An orchestrator is an omnibenchmark project which orchestrates the deployment, running and testing of each pieces of an Omnibenchmark. </p>"},{"location":"01_getting_started/01_module_contr/setup_module/04_submit/#integrate-a-module-to-an-orchestrator","title":"Integrate a module to an orchestrator","text":"<p>The list of current Omnibenchmarks and their related orchestrator can be found on the Omnibenchmark dashboard: </p> <ul> <li> <p>Omni-batch orchestrator</p> </li> <li> <p>Omniclustering orchestrator</p> </li> <li> <p>Omniclustering (pinned versions) orchestrator</p> </li> <li> <p>Omni spatial clustering orchestrator</p> </li> </ul> <p>To integrate your (populated and tested) module: </p> <ul> <li> <p>Follow the link to the relevant orchestrator</p> </li> <li> <p>Open a new issue and describe briefly the aim of your module (data/ method/ metric module ?) and provide a link to it</p> </li> <li> <p>The development team will check your module and integrate it in the Orchestrator worfkflow. When it is done, you will be able to view the result of your module on the shiny app. </p> </li> </ul>"},{"location":"01_getting_started/02_benchmarker/","title":"Index","text":""},{"location":"01_getting_started/02_benchmarker/#benchmarker","title":"Benchmarker","text":"<p>Generating new benchmarks can be automated using:</p> <ul> <li>Project templates, a semi-manual approach;</li> <li>omnibus, a CLI-like experience using gitlab API calls to terraform new benchmarks.</li> </ul>"},{"location":"01_getting_started/02_benchmarker/#setting-up-a-benchmark-with-omnibus","title":"Setting up a benchmark with <code>omnibus</code>","text":"<p>After installing omnibus, docker and other requirements (such as defining a gitlab token so our user can create new projects via gitlab API), as described in omnibus, we need to define the benchmark structure. This is done via config files, and could be wrapped by using omnibus' wrapper:</p> <pre><code>source(\"R/create_config.R\")\n\ncreate_config(\n    benchmark = \"Your_benchmark\",\n    config = \"config_Your_benchmark\",\n    datanames = c(\"name1\", \"name2\", \"name3\"),\n    methodnames = c(\"name1\", \"name2\"),\n    metricnames = c(\"name1\", \"name2\"),\n    explainer = TRUE, # Adds explanation of each config parameter to the buttom of the file\n    ... # Add any config parameter setting (does not work with BENCHMARK, REPONAMES,\n    # KEYWORDS, or TEMPLATES). Names must match the names from the template.\n    )\n# If you don't specific the config name, it will be called \"config_&lt;benchmark&gt;\".\n</code></pre> <p>Using this file, the command</p> <pre><code>omnibus -o -p -s -c CONFIGFILE\n</code></pre> <p>triggers the benchmark terraforming.</p> <p>To avoid storing the API token in clear text, you can pass it to <code>omnibus</code> with <code>--token TOKEN</code>.</p>"},{"location":"01_getting_started/03_method_user/","title":"Benchmark outputs","text":"<p>We provide benchmark outputs as datasets within each benchmark's gitlab modules.</p> <p>As for interactively browsing results, we provide bettr endpoints for all current benchmarks at http://bettr.omnibenchmark.org/ , including:</p> <ul> <li>https://bettr.omnibenchmark.org/DuoSCClustering2018/</li> <li>https://bettr.omnibenchmark.org/iris/</li> <li>https://bettr.omnibenchmark.org/omni_batch/</li> <li>https://bettr.omnibenchmark.org/omniclustering/</li> </ul>"},{"location":"02_advanced/","title":"Index","text":""},{"location":"02_advanced/#advanced-topics","title":"Advanced topics","text":"<ul> <li> <p>Module contributor: better control inputs, outputs and scripts. </p> </li> <li> <p>Benchmarker: more complex designs and better control how your benchmark is run and populated.</p> </li> <li> <p>Method user: download the results of a benchmark on your computer and wrangle the shiny app to your needs. </p> </li> <li> <p>Administrator: guides to set up nonmandatory omnibenchmark components (i.e. your own gitlab runners, triplestore etc)</p> </li> </ul>"},{"location":"02_advanced/01_module_contr/","title":"Index","text":""},{"location":"02_advanced/01_module_contr/#advances-topics-for-module-contributor","title":"Advances topics for module contributor","text":"<ul> <li>The omni_object instance and its classes</li> </ul>"},{"location":"02_advanced/01_module_contr/omni_object/","title":"Omnibenchmark classes","text":"<p>This section describes classes to manage omnibenchmark modules and their interactions. The main class is the <code>OmniObject</code>, which consollidates all relevant information and functions of a module. This object has further subclasses that define inputs, outputs, commands and the workflow.</p>"},{"location":"02_advanced/01_module_contr/omni_object/classes/01_omni_object/","title":"OmniObject","text":"<p>Main class to manage an omnibenchmark module. It takes the following arguments:</p> <ul> <li><code>name (str)</code>: Module name</li> <li><code>keyword (Optional[List[str]], optional)</code>: Keyword associated to the modules output dataset.</li> <li><code>title (Optional[str], optional)</code>: Title of the modules output dataset.</li> <li><code>description (Optional[str], optional)</code>: Description of the modules output dataset.</li> <li><code>script (Optional[PathLike], optional)</code>: Script to generate the modules workflow for.</li> <li><code>command (Optional[OmniCommand], optional)</code>: Workflow command - will be automatically generated if missing.</li> <li><code>inputs (Optional[OmniInput], optional)</code>: Definitions of the workflow inputs.</li> <li><code>parameter (Optional[OmniParameter], optional)</code>: Definitions of the workflow parameter.</li> <li><code>outputs (Optional[OmniOutput], optional)</code>: Definitions of the workflow outputs.</li> <li><code>omni_plan (Optional[OmniPlan], optional)</code>: The workflow description.</li> <li><code>benchmark_name (Optional[str], optional)</code>: Name of the benchmark the module is associated to.</li> <li><code>orchestrator (Optional[str], optional)</code>: Orchestrator url of the benchmark th emodule is associated to. Automatic detection.</li> <li><code>wflow_name (Optional[str], optional)</code>: Workflow name. Will be set to the module name if none.</li> <li><code>dataset_name (Optional[str], optional)</code>: Dataset name. Will be set to the module name if none.</li> </ul> <p>The following class methods can be run on an instance of an OmniObject:</p> <ul> <li><code>create_dataset()</code>: Method to create a renku dataset with the in the object specified attributes in the current renku project.</li> <li><code>update_object()</code>: Method to check for new imports or updates in input and the parameter datasets. Will update object attributes accordingly.</li> <li><code>run_renku()</code>: Method to generate and update the workflow and all output files as specified in the object.</li> <li><code>update_result_dataset()</code>: Method to update and add all output datasets to the dataset specified in the object.</li> </ul>"},{"location":"02_advanced/01_module_contr/omni_object/classes/02_omni_input/","title":"OmniInput","text":"<p>Class to manage inputs of an omnibenchmark module. This class has the following attributes:</p> <ul> <li><code>names (List[str])</code>: Names of the input filetypes</li> <li><code>prefix (Optional[Mapping[str, List[str]]], optional)</code>: Prefixes (or substrings) of the input filetypes.</li> <li><code>input_files (Optional[Mapping[str, Mapping[str, str]]], optional)</code>: Input files ordered by file types.</li> <li><code>keyword (Optional[List[str]], optional)</code>: Keyword to define which datasets are imported as input datasets.</li> <li><code>default (Optional[str], optional)</code>: Default input name (e.g., dataset).</li> <li><code>filter_names (Optional[List[str]], optional)</code>: Input dataset names to be ignored.</li> </ul> <p>The following class methods can be run on an instance of an OmniInput:</p> <ul> <li><code>update_inputs()</code>: Method to import new and update existing input datasets and update the object accordingly</li> </ul>"},{"location":"02_advanced/01_module_contr/omni_object/classes/03_omni_parameter/","title":"OmniParameter","text":"<p>Class to manage parameter of an omnibenchmark module. This class has the following attributes:</p> <ul> <li><code>names (List[str])</code>: Name of all valid parameter</li> <li><code>values (Optional[Mapping[str, List]], optional)</code>: Parameter values - usually automatically detected.</li> <li><code>default (Optional[Mapping[str, str]], optional)</code>: Default parameter values.</li> <li><code>keyword (Optional[List[str]], optional)</code>: Keyword to import the parameter dataset with.</li> <li><code>filter (Optional[Mapping[str, str]], optional)</code>: Filter to use for the parameter space.</li> <li><code>combinations (Optional[List[Mapping[str, str]]], optional)</code>: All possible parameter combinations.</li> </ul> <p>The following class methods can be run on an instance of an OmniInput:</p> <ul> <li><code>update_parameter()</code>: Method to import and update parameter datasets and update the object/parameter space accordingly.</li> </ul>"},{"location":"02_advanced/01_module_contr/omni_object/classes/04_omni_output/","title":"OmniOutput","text":"<p>Class to manage outputs of an omnibenchmark module. This class has the following attributes:</p> <ul> <li><code>name (str)</code>: Name that is specific for all outputs. Typically the module name/OmniObject name.</li> <li><code>out_names (List[str])</code>: Names of the output file types</li> <li><code>output_end (Optional[Mapping[str, str]], optional)</code>: Endings of the output filetypes.</li> <li><code>out_template (str, optional)</code>: Template to generate output file names.</li> <li><code>file_mapping (Optional[List[OutMapping]], optional)</code>: Mapping of input files, parameter values and output files.</li> <li><code>inputs (Optional[OmniInput], optional)</code>: Object specifying all valid inputs.</li> <li><code>parameter (Optional[OmniParameter], optional)</code>: Object speccifying the parameter space.</li> <li><code>default (Optional[Mapping], optional)</code>: Default output files.</li> <li><code>filter_json(Optional[str], optional)</code>: Path to json file with filter combinations.</li> <li><code>template_fun (Optional[Callable[..., Mapping]], optional)</code>: Function to use to automatically generate output filenames.</li> <li><code>template_vars (Optional[Mapping], optional)</code>: Variables that are used by template_fun.</li> </ul> <p>The following class methods can be run on an instance of an OmniInput:</p> <ul> <li><code>update_outputs()</code>: Method to update the output definitions according to the objects attributes.</li> </ul>"},{"location":"02_advanced/01_module_contr/omni_object/classes/05_omni_command/","title":"OmniCommand","text":"<p>Class to manage the main workflow command of an omnibenchmark module. This class has the following attributes:</p> <ul> <li><code>script (Union[PathLike, str])</code>: Path to the script run by the command</li> <li><code>interpreter (str, optional)</code>: Interpreter to run the script with.</li> <li><code>command_line (str, optional)</code>: Command line to be run.</li> <li><code>outputs (OmniOutput, optional)</code>: Object specifying all outputs.</li> <li><code>input_val (Optional[Mapping], optional)</code>: Input file tyoes and paths to run the command on.</li> <li><code>parameter_val (Optional[Mapping], optional)</code>: Parameter names and values to run the command with.</li> </ul> <p>The following class methods can be run on an instance of an OmniInput:</p> <ul> <li><code>update_command()</code>: Method to update the command according to the outputs,inputs,parameter.</li> </ul>"},{"location":"02_advanced/01_module_contr/omni_object/classes/06_omni_plan/","title":"OmniPlan","text":"<p>Class to manage the workflow of an omnibenchmark module. This class has the following attributes:</p> <ul> <li><code>plan (PlanViewModel)</code>: A plan view model as defined in renku</li> <li><code>param_mapping (Optional[Mapping[str, str]], optional)</code>: A mapping between the component names of the plan and the OmniObject.</li> </ul> <p>The following class methods can be run on an instance of an OmniInput:</p> <ul> <li><code>predict_mapping_from_file_dict()</code>: Method to predict the mapping from the (input-, output-, parameter) file mapping used to generate the command.</li> </ul>"},{"location":"02_advanced/02_benchmarker/","title":"Advanced guides for benchmarker","text":""},{"location":"02_advanced/03_method_user/","title":"Index","text":""},{"location":"02_advanced/03_method_user/#advanced-topics-for-method-user","title":"Advanced topics for method user","text":""},{"location":"02_advanced/04_admin/","title":"Index","text":""},{"location":"02_advanced/04_admin/#administering-omnibenchmark","title":"Administering omnibenchmark","text":""},{"location":"02_advanced/04_admin/#components","title":"Components","text":"<p>Omnibenchmark is designed as a SaaS. Omnibenchmark's components are modular, and some you can deploy in your own to have extra control. omnibenchmark.org already provides all these components, so this guide only matters if you'd like to replace some of the <code>default</code> components.</p> <ul> <li>renkulab and gitlab deployment<ul> <li><code>default</code>: renkulab and gitlab deployment from renkulab.io.</li> <li><code>self</code>: k8s and following renku's admin docs.</li> </ul> </li> <li>gitlab runners <ul> <li><code>default</code>: provided by renkulab.io.</li> <li><code>self</code>: registered runners in any architecture (laptops, HPC, GPU-powered machines etc).</li> </ul> </li> <li>omnibenchmark triplestore<ul> <li><code>default</code>: apache jena from robinsonlab (ask us for details).</li> <li><code>self</code>: deploy a jena/fuseki to have full control on your triples.</li> </ul> </li> <li>centralized benchmark listing/json.<ul> <li><code>default</code>: robinsonlab (ask us for details, queried by omb-py).</li> </ul> </li> <li>bettr deployment <ul> <li><code>default</code>: shiny-server by robinsonlab (ask us for details).</li> <li><code>self</code>: set up a shiny-server (perhaps using singularity).</li> </ul> </li> <li>Web dashboard<ul> <li><code>default</code>: omnibenchmark.org from robinsonlab (ask us for details, code).</li> </ul> </li> </ul> <p>Besides, omnibenchmark relies on a set of python modules and R packages, which also need to be cross-compatible and compatible with your renkulab deployment, including:</p> <ul> <li>omnibenchmark python<ul> <li>tip: needs to be compatible with renku python</li> </ul> </li> <li>omni validator</li> <li>omni_cli</li> <li>bettr</li> </ul>"},{"location":"02_advanced/04_admin/#configuration-guides","title":"Configuration guides","text":"<p>Mind some of our docs are missing; drafts are listed as \u2713, ready-to-use docs as  \u2705.</p> <ol> <li>Services overview  \u2713</li> <li>Start a new benchmark \u2713</li> <li>Set up a triplestore </li> <li>Register a runner \u2705</li> <li>Serve bettr \u2705</li> <li>Serve a dashboard </li> <li>Deploy renkulab </li> </ol>"},{"location":"02_advanced/04_admin/#contact","title":"Contact","text":"<p>To get tokens/authentication details: mark.robinson@uzh.ch, izaskun.mallona@uzh.ch</p>"},{"location":"02_advanced/04_admin/01_services/","title":"Services overview","text":""},{"location":"02_advanced/04_admin/01_services/#services-dependencies","title":"Services dependencies","text":"<p>Several components, mainly omnibenchmark python, query other components (APIs, triplestore) on runtime.</p> <ul> <li>Renku API<ul> <li>omnibenchmark python</li> </ul> </li> <li>Gitlab API<ul> <li>omnibenchmark python</li> <li>gitlab runners</li> </ul> </li> <li>Triplestore<ul> <li>omnibenchmark python (query)</li> <li>omnibenchmark python (population)</li> <li>anywhere (population, from within the CI/CD job; update token needed)</li> <li>(not yet implemented) git/renku hooks</li> </ul> </li> <li>Metric results<ul> <li>a cron job from the machine serving bettr deployments </li> </ul> </li> </ul>"},{"location":"02_advanced/04_admin/01_services/#tips","title":"Tips","text":"<p>You can run renku projects in <code>renku stealth mode</code> disabling the <code>https://renkulab.io/webhooks/events</code> hook within your gitlab project by browsing <code>Settings -&gt; Webhooks</code>.</p>"},{"location":"02_advanced/04_admin/02_new_benchmark/","title":"Start a new benchmark","text":"<p>This guide relies on the <code>default</code> omnibenchmark components (gitlab runners, triplestore etc).</p> <p>GUI click/fill instructions have been tested in https://gitlab.renkulab.io running GitLab Community Edition 14.10.5 and might change in future releases.</p>"},{"location":"02_advanced/04_admin/02_new_benchmark/#foreground","title":"Foreground","text":"<p>The starting point for a non automated benchmark creation is renkulab.io.</p> <p>Interestingly, renkulab.io's gitlab is available at gitlab.renkulab.io. If you log in to renkulab.io you'll be logged in to the gitlab too. To switch easily from renku's GUI to gitlab's GUI please notice the <code>projects</code> or <code>gitlab</code> components of these URLs: </p> <ul> <li>https://gitlab.renkulab.io/omnibenchmark/iris_example/iris-dataset </li> </ul> <p>vs</p> <ul> <li>https://renkulab.io/projects/omnibenchmark/iris_example/iris-dataset</li> </ul> <p>Both refer to the same repository.</p>"},{"location":"02_advanced/04_admin/02_new_benchmark/#gitlab-group-and-subgroups","title":"Gitlab group (and subgroups)","text":"<p>Repository groups can be created by browsing https://gitlab.renkulab.io pressing <code>new group</code>. Repository subgroups can be created by browsing a group, i.e.  https://gitlab.renkulab.io/omb_benchmarks , and pressing <code>new subgroup</code>. . If interested in creating a subgroup below 'known' omnibenchmark groups such as https://gitlab.renkulab.io/omb_benchmarks  or https://gitlab.renkulab.io/omnibenchmark your user needs to be granted rights; contact the omnibecnhmark team if so.</p> <p>Tip: you can add other people to your benchmark group/subgroup by pressing (left panel) <code>(sub)group information -&gt; Members</code>.</p> <p>Tip: it's advisable to register a dedicated gitlab runner when generating a group, and use it as a group runner for CI/CD. For that, check our runner's docs.</p>"},{"location":"02_advanced/04_admin/02_new_benchmark/#benchmark-masked-variablestokens","title":"Benchmark masked variables/tokens","text":"<p>Once you've created the group or subgroup where your benchmark will leave (or at least your orchestrator will), you'll need to create a token with <code>api_read</code> scope. For that, visit your group <code>Settings -&gt; CI/CD -&gt; Variables</code> setting and create a nonprotected, masked variable, for instance `OMB_ACCESS_TOKEN~. And keep its content stored somewhere for future usage. </p>"},{"location":"02_advanced/04_admin/02_new_benchmark/#user-tokens","title":"User tokens","text":"<p>A personal gitlab token will allow to automate actions. To generate one: - log in at https://gitlab.renkulab.io - visit https://gitlab.renkulab.io/-/profile/personal_access_tokens - create one with the adequate scope (i.e. read API)</p> <p>This token won't be used by omnibenchmark, but can be handy to have.</p>"},{"location":"02_advanced/04_admin/02_new_benchmark/#orchestrator","title":"Orchestrator","text":"<p>The core component of omnibenchmark is an orchestrator, which stitches together datasets, methods and metrics. Without an orchestrator there is no benchmark. Still, you can set up the orchestrator last.</p> <p>Orchestrators are unusual omnibenchmark components. They're mainly a gitlab CI/CD yaml. An example orchestrator looks like this:</p> <pre><code>variables:\n  GIT_STRATEGY: fetch\n  GIT_SSL_NO_VERIFY: \"true\"\n  GIT_LFS_SKIP_SMUDGE: 1\n  OMNIBENCHMARK_NAME: iris_example\n  TRIPLESTORE_URL: http://imlspenticton.uzh.ch/omni_iris_data\n  CI_UPSTREAM_PROJECT: ${CI_PROJECT_PATH}\n  OMNI_UPDATE_TOKEN: ${OMNI_UPDATE_TOKEN}\n  CI_PUSH_TOKEN: ${CI_PUSH_TOKEN}\n\nstages:\n  - build\n  - data_run\n  - process_run\n  - parameter_run\n  - method_run\n  - metric_run\n  - summary_metric_run\n\nimage_build:\n  stage: build\n  image: docker:stable\n  rules: \n    - if: '$CI_PIPELINE_SOURCE == \"pipeline\"'\n      when: never\n  before_script:\n    - docker login -u gitlab-ci-token -p $CI_JOB_TOKEN http://$CI_REGISTRY\n  script: |\n    CI_COMMIT_SHA_7=$(echo $CI_COMMIT_SHA | cut -c1-7)\n    docker build --tag $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA_7 .\n    docker push $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA_7\n\ntrigger_iris_dataset:\n  stage: data_run \n  only:\n    - schedules\n  trigger: \n    project: omnibenchmark/iris_example/iris-dataset\n    strategy: depend\n</code></pre> <p>The first stanza (<code>variables</code>) defines variables needed for overall git behaviour as well as helping with authentication. The important tokens are:</p> <ul> <li>OMNI_UPDATE_TOKEN</li> <li>CI_PUSH_TOKEN</li> </ul> <p>The second stanza (<code>image_build</code>) generates a renku-powered docker image (i.e. for interactive sessions).</p> <p>The third stanza (<code>trigger_iris_dataset</code>) is how most of the orchestrator CI/CD tasks look like: they trigger downstream projects CI/CDs; that is, their <code>gitlab-ci.yaml</code>. In the example above, triggers the iris dataset CI/CD.</p>"},{"location":"02_advanced/04_admin/02_new_benchmark/#terraforming-via-templates","title":"Terraforming via templates","text":"<p>Omnibenchmark's renku templates help to create new benchmark components.</p>"},{"location":"02_advanced/04_admin/02_new_benchmark/#terraforming-via-gitlab-api","title":"Terraforming via gitlab API","text":"<p>omnibus helps automating benchmark creation.</p>"},{"location":"02_advanced/04_admin/02_new_benchmark/#tipsresources","title":"Tips/resources","text":"<ul> <li>Predefined CI/CD variables</li> </ul>"},{"location":"02_advanced/04_admin/02_new_benchmark/#background","title":"Background","text":"<p>Infrastructure-wise, some manual actions need to be done to start a new benchmark.</p> <ul> <li>register runner(s)</li> <li>set up bettr endpoint</li> <li>add a triplestore dataset</li> <li>add a triplestore apache reverse proxy</li> </ul>"},{"location":"02_advanced/04_admin/02_new_benchmark/#runners","title":"Runners","text":"<p>It's advisable to register a dedicated gitlab runner when generating a benchmark, and use it as a group runner for CI/CD. For that, check our runner's docs.</p>"},{"location":"02_advanced/04_admin/03_triplestore/","title":"Set up a triplestore","text":""},{"location":"02_advanced/04_admin/03_triplestore/#setting-up-jenafuseki","title":"Setting up jena/fuseki","text":"<p>Lorem ipsum</p>"},{"location":"02_advanced/04_admin/04_runner/","title":"Register a runner","text":""},{"location":"02_advanced/04_admin/04_runner/#install-gitlab-runner-in-your-linux-machine","title":"Install gitlab-runner in your (linux) machine","text":"<p>For CPU (not GPU) computing, machines to run group gitlab-runners with docker executors need docker and the gitlab-runner software installed. The machine (server, laptop) running the runners does not need a public IP.</p>"},{"location":"02_advanced/04_admin/04_runner/#docker","title":"docker","text":"<p>Assuming your system is <code>apt</code>-based (debian, ubuntu):</p> <pre><code>sudo apt-get update\n\nsudo apt install -y apt-transport-https ca-certificates curl gnupg2 software-properties-common\n\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\n\nsudo add-apt-repository \\\n   \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\\n   $(lsb_release -cs) \\\n   stable\"\n\nsudo apt update\n\nsudo apt-get install -y docker-ce docker-ce-cli containerd.io\n\nsystemctl status docker #; start if needed\n\ngroupadd docker\nusermod -aG docker YOURUSER\n</code></pre>"},{"location":"02_advanced/04_admin/04_runner/#gitlab-runner","title":"gitlab-runner","text":"<p>Mind the <code>arch</code>itecture of your machine; amd64 assumed here (Apple M1s: <code>arm64</code>)</p> <pre><code>mkdir -p ~/soft/gitlab-runner; cd $_\n\narch='amd64'\ncurl -LJO \"https://gitlab-runner-downloads.s3.amazonaws.com/latest/deb/gitlab-runner_${arch}.deb\"\nsudo dpkg -i gitlab-runner_amd64.deb\n\nsudo gitlab-runner start\n\n# create an user too\nsudo useradd --comment 'GitLab Runner' --create-home gitlab-runner --shell /bin/bash\n</code></pre>"},{"location":"02_advanced/04_admin/04_runner/#register-a-gitlab-runner-in-your-linux-machine","title":"Register a gitlab runner in your (linux) machine","text":"<p>On GitLab Community Edition 14.10.5 (might sligthly vary within versions) visit your repository or group of repositories, i.e. https://gitlab.renkulab.io/omb_benchmarks, go to <code>CI/CD</code> -&gt; <code>Runners</code> (caution, not to <code>Settings -&gt; CI/CD</code>, click <code>Register a group runner</code>, copy to the clipboard the registration token.</p> <p>Let's assume the token is <code>94daGGiXCgwthisisnotarealtoken</code>.</p> <p>In your gitlab-runner machine, run:</p> <pre><code>REGISTRATION_TOKEN=\"94daGGiXCgwthisisnotarealtoken\"\n\nRUNNER_NAME=examplerunner\nEXECUTOR=\"docker\"\n\nsudo gitlab-runner register \\\n  --non-interactive \\\n  --url \"https://gitlab.renkulab.io\" \\\n  --registration-token \"$REGISTRATION_TOKEN\" \\\n  --description \"$RUNNER_NAME\" \\\n  -locked=false  \\\n  --run-untagged=true \\\n  --executor \"$EXECUTOR\" \\\n  --description \"$RUNNER_NAME\" \\\n  --docker-image docker:stable \\\n  --docker-network-mode \"host\" \\\n  --docker-volumes /var/run/docker.sock:/var/run/docker.sock\n</code></pre> <p>To check whether the gitlab runner is running,</p> <pre><code>sudo gitlab-runner list\nsudo journalctl -u gitlab-runner\n</code></pre>"},{"location":"02_advanced/04_admin/04_runner/#tips-and-troubleshooting","title":"Tips and troubleshooting","text":"<p>To inspect or edit/finetune the concurrency, timeout behaviour, and/or each runners details can be checked at <code>/etc/gitlab-runner/config.toml</code>. An example config file is:</p> <pre><code>concurrent = 10\ncheck_interval = 0\n\n[session_server]\n  session_timeout = 36000\n\n[[runners]]\n  name = \"tesuto-robinsonlab-gitlab-docker\"\n  url = \"https://gitlab.renkulab.io\"\n  token = \"YfztssssssssssssssN\"\n  executor = \"docker\"\n  [runners.custom_build_dir]\n  [runners.cache]\n    [runners.cache.s3]\n    [runners.cache.gcs]\n    [runners.cache.azure]\n  [runners.docker]\n    tls_verify = false\n    image = \"docker:stable\"\n    privileged = false\n    disable_entrypoint_overwrite = false\n    oom_kill_disable = false\n    disable_cache = false\n    volumes = [\"/var/run/docker.sock:/var/run/docker.sock\", \"/cache\"]\n    network_mode = \"host\"\n    shm_size = 0\n\n[[runners]]\n  name = \"iris-tesuto-robinsonlab-gitlab-docker\"\n  url = \"https://gitlab.renkulab.io\"\n  token = \"xbxxxxxxxxxxxxxKi\"\n  executor = \"docker\"\n  [runners.custom_build_dir]\n  [runners.cache]\n    [runners.cache.s3]\n    [runners.cache.gcs]\n    [runners.cache.azure]\n  [runners.docker]\n    tls_verify = false\n    image = \"docker:stable\"\n    privileged = false\n    disable_entrypoint_overwrite = false\n    oom_kill_disable = false\n    disable_cache = false\n    volumes = [\"/var/run/docker.sock:/var/run/docker.sock\", \"/cache\"]\n    network_mode = \"host\"\n    shm_size = 0\n</code></pre> <p>Double check <code>privileged = false</code>, needs to be false. Re: the OOM killer, TLS verify etc: up to you.</p> <p><code>concurrent</code> limits how many jobs can run concurrently, across all registered runners; beware gitlab-runner does not know how many cores does each job use. For mostly single-core jobs defining <code>concurrent</code> as your machine's <code>nproc</code> - 2 should work.</p> <p>Very useful advanced config.toml docs.</p> <p>Mind that your gitlab repositories list their registered runners at <code>Settings -&gt; CI/CD -&gt; Runners</code>. Beware of the <code>shared runners</code>, <code>group runners</code> (the ones we're configuring here) and <code>other available runners</code>; disable the ones you don't want to use. </p> <p>Shell executors are also easy to config. Docker-in-docker is not working well due to subpar caching.</p> <p>Remember to remove old images and vacuum/clean your machine (with a cron job). A recipe to prune old images, containers, networks and volumes (to be cron-ed at midnight):</p> <pre><code>#!/bin/bash\n##\n## Prunes images, containers, networks, volumes (ideally daily, at midnight)\n## Does some convoluted checks to keep the most up-to-date image\n## largely untested\n##\n## 18 Aug 2022\n## Izaskun Mallona\n## GPLv3\n\n# prune images older than 24h\n## the -a means it removes dangling but also those not used by existing containers\n# the until-24h means removes those older than 24h\n\ndocker network prune  --filter \"until=200h\" -f\n\ndocker volume prune  -f \"until=200h\"\n\nfor diru in $(docker images -a --format \"{{.Repository}}\" | sort | uniq)\ndo\n    ## sort by the timestamp, list all except the first/most recent\n    for old_thing in $(docker images -a --format \\\n        \"{{.ID}}\\t{{.Size}}\\t{{.Repository}}\\t{{.CreatedAt}}\" | \\\n        grep $diru | \\\n        sort -k4r | \\\n        tail -n+2 | \\\n        cut -f1)\n    do\n        echo Removing $old_thing from $diru\n        docker rmi $old_thing;\n    done\ndone\n\n#remove dangling\ndocker image prune -f\n</code></pre>"},{"location":"02_advanced/04_admin/05_bettr/","title":"Serve bettr","text":"<p>We are currently serving bettr via shiny-server within singularity. So this recipe aims to install singularity on a linux machine, get the bettr image, and set up a cron to retrieve metrics.</p> <p>The host needs to face the internet. And a firewall.</p> <p>We are aware rstudioconnect could simplify this, or just running <code>bettr</code> locally.</p>"},{"location":"02_advanced/04_admin/05_bettr/#installs","title":"Installs","text":""},{"location":"02_advanced/04_admin/05_bettr/#singularity","title":"singularity","text":"<p>We assume an <code>apt</code>-apt distribution, i.e. debian or ubuntu. <pre><code>sudo apt-get update\n\nsudo apt-get update &amp;&amp; sudo apt-get install -y \\\n    build-essential \\\n    uuid-dev \\\n    libgpgme-dev \\\n    squashfs-tools \\\n    libseccomp-dev \\\n    wget \\\n    pkg-config \\\n    git \\\n    cryptsetup-bin \\\n    libgpgme11-dev\n\nmkdir -p ~/soft/go\ncd $_\n\nexport VERSION=1.13.5 OS=linux ARCH=amd64 &amp;&amp; \\\n    wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz\n\nsudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz\n\necho 'export GOPATH=${HOME}/go' &gt;&gt; ~/.bashrc &amp;&amp; \\\n    echo 'export PATH=/usr/local/go/bin:${PATH}:${GOPATH}/bin' &gt;&gt; ~/.bashrc &amp;&amp; \\\n    source ~/.bashrc\n\nmkdir -p ~/soft/singularity\ncd $_\n\nexport VERSION=3.8.3 &amp;&amp; # adjust this as necessary \\\n    wget https://github.com/sylabs/singularity/releases/download/v3.8.3/singularity-ce-3.8.3.tar.gz\n\ntar -xzf singularity-*${VERSION}.tar.gz\n\ncd sing*\n\n./mconfig &amp;&amp; \\\n    make -C ./builddir\n\nsudo make -C ./builddir install\n\n# test\n\nsingularity exec library://alpine cat /etc/alpine-release\n</code></pre></p>"},{"location":"02_advanced/04_admin/05_bettr/#apache","title":"apache","text":"<pre><code>sudo apt install apache2\n</code></pre> <p>Also, configure iptables. Or ufw, open port 80/443, 22, and whatever the port the bettr image is going to use below.</p>"},{"location":"02_advanced/04_admin/05_bettr/#bettr-image","title":"bettr image","text":"<p>The bettr image is generated by https://renkulab.io/gitlab/omnibenchmark/bettr-deployer .</p>"},{"location":"02_advanced/04_admin/05_bettr/#serving-the-bettr-app","title":"serving the bettr app","text":"<p>To read the bettr image from the registry, an user token with <code>read_registry</code> power is needed; below encoded as <code>INGULARITY_DOCKER_PASSWORD</code>.</p> <pre><code>mkdir -p ~/bettr_deployer/apps ~/bettr_deployer/logs ~/bettr_deployer/lib ~/bettr_deployer/tmp\n\ncd ~/bettr_deployer\nexport SINGULARITY_DOCKER_USERNAME='izaskun.xx@xxxuzh.ch'\nexport SINGULARITY_DOCKER_PASSWORD='KxasgasgasgxK' # read_registry granted\nexport NAMESPACE=\"omnibenchmark\"\nexport ID=\"obm_bettr\"\nexport VERSION=\"1d0b31b\"\n\nsingularity instance start --env SHINY_LOG_STDERR=1 \\\n            --bind ./apps:/srv/shiny-server/bettr \\\n            --bind ./log:/var/log/shiny-server_bettr \\\n            --bind ./lib:/var/lib/shiny-server \\\n            --bind ./tmp:/tmp \\\n            bettr-deployer_\"$ID\"-\"$VERSION\".sif \"$ID\"_\"$VERSION\"\n\nsingularity exec instance://\"$ID\"_\"$VERSION\" \"shiny-server\" &amp;\n</code></pre> <p>So apps will be served if placed at ~/bettr_deployer/apps, i.e.:</p> <pre><code>/home/shiny/bettr_deployer/apps/omni_clustering/:\ntotal 12\n-rw-rw-r-- 1 shiny shiny 1614 Jul 27 13:28 app.R\ndrwxrwxr-x 2 shiny shiny 4096 Sep  1  2022 data\n-rw-rw-r-- 1 shiny shiny    0 Sep  1  2022 restart.txt\n\n/home/shiny/bettr_deployer/apps/omni_clustering/data:\ntotal 340\n-rw-rw-r-- 1 shiny shiny 347951 Jan 19  2023 summary.json\n</code></pre> <p>where <code>app.R</code> contains</p> <pre><code>#!/usr/bin/env R\n\n## Retrieve data\n\n## make recognize the argument as the data that has been just downloaded, for FAIR\nsuppressPackageStartupMessages({\n  library(\"bettr\")\n})\n\nresDir   &lt;- 'data'\nbstheme  &lt;- 'darkly'\nappTitle &lt;- 'bettr'\nmetrics  &lt;- 'all'\n\nres_files &lt;- list.files(path = paste0(resDir), full.names = TRUE)\n\n## Read result files\nout &lt;- jsonlite::read_json(res_files, simplifyVector = TRUE)\n\n## reconverting true false to logical\ncolnames(out$metricInfo) &lt;- c(\"Metric\", \"Group\")\nout$initialTransforms &lt;- lapply(out$initialTransforms, function(x){\n  lapply(x, function(y){\n    as.logical(y)\n  })\n})\n\n# replacing na value\nout$idInfo[is.na(out$idInfo)] &lt;- \"NaN\"\n\n# call bettr\nbettr(\n  df = out$df,\n  idCol = if (is.null(out$idCol)) {\n    \"Method\"\n  } else {\n    out$idCol\n  },\n  metrics = if (is.null(out$metrics)){\n    setdiff(colnames(out$df), out$idCol)\n  } else {\n    out$metrics\n  },\n  initialTransforms = if (is.null(out$initialTransforms)) {\n    list()\n  } else {\n    out$initialTransforms\n  },\n  metricInfo = out$metricInfo,\n  metricColors = out$metricColors,\n  idInfo = out$idInfo,\n  idColors = out$idColors,\n  weightResolution = if (is.null(out$weightResolution)) {\n    0.05\n  } else {\n    out$weightResolution\n  },\n  bstheme = if (is.null(out$bstheme)) {\n    \"darkly\"\n  } else {\n    out$bstheme\n  },\n  appTitle = if (is.null(out$appTitle)) {\n    \"bettr\"\n  } else {\n    out$appTitle\n</code></pre> <p>And the data is either a fixed snapshot (or retrieved periodically via cron) of a suitably formated metrics file, like this one.</p>"},{"location":"02_advanced/04_admin/05_bettr/#dmz-related-maintenance","title":"DMZ-related maintenance","text":""},{"location":"02_advanced/04_admin/05_bettr/#firewall-shiny-ports","title":"firewall / shiny ports","text":"<p>Mind to open port 3840 to fit the docker container shiny-server's port, or update the Dockerfile or the singularity port mapping as needed.</p>"},{"location":"02_advanced/04_admin/05_bettr/#cron-to-retrieve-results","title":"cron to retrieve results","text":"<p>Using the clustering benchmark as an example: </p> <pre><code>cd /home/shiny/bettr_deployer/apps/omni_clustering/data\n\nwget -q https://renkulab.io/gitlab/omnibenchmark/omni_clustering/omni_clustering_summary/-/raw/main/data/omni_clustering_summary/omni_clustering_summary.json -O summary.json\n\n## to make sure the shiny app reloads\ntouch ../app.R\n</code></pre>"},{"location":"02_advanced/04_admin/05_bettr/#tips-and-troubleshooting","title":"Tips and troubleshooting","text":"<ul> <li>(internal) The unusual shiny's UID 998 avoids a collision with robinsonlab's 999, whic is the rstudio-server user.</li> </ul>"},{"location":"02_advanced/04_admin/06_dashboard/","title":"Serve the dashboard","text":"<p>Lorem ipsum.</p>"},{"location":"02_advanced/04_admin/07_renkulab/","title":"Deploy renkulab","text":"<p>Lorem ipsum.</p>"},{"location":"03_howto/","title":"Index","text":""},{"location":"03_howto/#how-tos","title":"How to's","text":""},{"location":"03_howto/#build-object","title":"Build object","text":"<ul> <li>Objects</li> </ul>"},{"location":"03_howto/#create-dataset","title":"Create dataset","text":"<ul> <li>Datasets</li> </ul>"},{"location":"03_howto/#generate-workflow","title":"Generate workflow","text":"<ul> <li>Workflows</li> </ul>"},{"location":"03_howto/#filter","title":"Filter","text":"<ul> <li>Filter parameters and inputs in objects</li> </ul>"},{"location":"03_howto/00_config_yaml/","title":"The config.yaml file","text":"<p>Usually all specific information about a benchmark project can be specified in a <code>config.yaml</code> file. Below we show an example with all standard fields and explanations to them. Many fields are optional and do not apply to all modules. All unneccessary fields can be skipped. There are further optional fields for specfic edge cases, that are described in an extra <code>config.yaml</code> file. In general the <code>config.yaml</code> file consists of a data, an input, an output and a parameter section as well as a few extra fields to define the main benchmark script and benchmark type. Except for the data section the other sections are optional. Multiple values can be parsed as lists.</p> <pre><code># Data section to describe the object and the associated (result) dataset\ndata:\n# Name of the dataset\nname: \"out_dataset\"\n# Title of the dataset (Optional)\ntitle: \"Output of an example OmniObject\"\n# Description of the dataset (Optional)\ndescription: \"This dataset is supposed to store the output files from the example omniobject\"\n# Dataset keyword(s) to make this dataset reachable for other projects/benhcmark components\nkeywords: [\"example_dataset\"]\n# Script to be run by the workflow associated to the project\nscript: \"path/to/method/dataset/metric/script.py\"\n# Interpreter to run the script (Optional, automatic detection)\ninterpreter: \"python\"\n# Benchmark that the object is associated to.\nbenchmark_name: \"omni_celltype\"\n# Orchestrator url of the benchmark (Optional, automatic detection)\norchestrator: \"https://www.orchestrator_url.com\"\n# Input section to describe output file types. (Optional)\ninputs:\n# Keyword to find input datasets, that shall be imported \nkeywords: [\"import_this\", \"import_that\"]\n# Input file types\nfiles: [\"count_file\", \"dim_red_file\"]\n# Prefix (part of the filename is sufficient) to automatically detect file types by their names\nprefix:\ncount_file: \"counts\"\ndim_red_file: [\"features\", \"genes\"]\n# Output section to describe output file types. (Optional)\noutputs:\n# Output filetypes and their endings\nfiles:\ncorrected_counts: end: \".mtx.gz\"\nmeta:\nend: \".json\"\n# Parameter section to describe the parameter dataset, values and filter. (Optional)\nparameter:\n# Names of the parameter to use\nnames: [\"param1\", \"param2\"]\n# Keyword(s) used to import the parameter dataset\nkeywords: [\"param_dataset\"]\n# Filter that specify limits, values or combinations to exclude\nfilter:\nparam1:\nupper: 50\nlower: 3\nexclude: 12\nparam2:\n\"path/to/file/with/parameter/combinations/to/exclude.json\"\n</code></pre> <p>Specific fields, that are only relevant for edge cases. These fields have their counterparts in the generated {ref}<code>OmniObject &lt;section-classes&gt;</code>. Changes of the attributes of the OmniObject instance have the same effects, but come with the flexibility of python code. </p> <pre><code># Command to generate the workflow with (Optional, automatic detection)\ncommand_line: \"python path/to/method/dataset/metric/script.py --count_file data/import_this_dataset/...mtx.gz\"\ninputs:\n# Datasets and manual file type specifications (automatic detection!)\ninput_files:\nimport_this_dataset:\ncount_file: \"data/import_this_dataset/import_this_dataset__counts.mtx.gz\"\ndim_red_file: \"data/import_this_dataset/import_this_dataset__dim_red_file.json\"\n# (Dataset) name that default input files belong to (Optional, automatic detection)\ndefault: \"import_this_dataset\"\n# Input dataset names that should be ignored (even if they have one of the specified input keywords assciated)\nfilter_names: [\"data1\", \"data2\"]\noutputs:\n# Template to automatically generate output filenames (Optional - recommended for advanced user only)\ntemplate: \"data/${name}/${name}_${unique_values}_${out_name}.${out_end}\"\n# Variables used for automatic output filename generation (Optional - recommended for advanced user only)\ntemplate_vars:\nvars1: \"random\"\nvars2: \"variable\"\n# Manual specification of mapping for output files and their corresponding input files and parameter values (automatic detection!)\nfile_mapping:\nmapping1: output_files:\ncorrected_counts: \"data/out_dataset/out_dataset_import_this__param1_10__param2_test_corrected_counts.mtx.gz\"\nmeta: \"data/out_dataset/out_dataset_import_this__param1_10__param2_test_meta.json\"\ninput_files:\ncount_file: \"data/import_this_dataset/import_this_dataset__counts.mtx.gz\"\ndim_red_file: \"data/import_this_dataset/import_this_dataset__dim_red_file.json\"\nparameter:\nparam1: 10\nparam2: \"test\"\n# Default output files (Optional, automatic detection)\ndefault:\ncorrected_counts: \"data/out_dataset/out_dataset_import_this__param1_10__param2_test_corrected_counts.mtx.gz\"\nmeta: \"data/out_dataset/out_dataset_import_this__param1_10__param2_test_meta.json\"\nparameter:\ndefault:\nparam1: 10\nparam2: \"test\"\n</code></pre>"},{"location":"03_howto/01_build_object/","title":"Build an OmniObject from yaml","text":"<p>All relevant information on how to run a specific module are stored as {ref}<code>OmniObject &lt;section-omniobject&gt;</code>. The most convenient way to generate an instance of an <code>OmniObject</code> is to build it from a <code>config.yaml</code> file using the <code>get_omni_object_from_yaml()</code> function:</p> <pre><code>## modules\nfrom omnibenchmark.utils.build_omni_object import get_omni_object_from_yaml\n## Load object\nomni_obj = get_omni_object_from_yaml('src/config.yaml')\n</code></pre>"},{"location":"03_howto/02_create_dataset/","title":"Datasets","text":""},{"location":"03_howto/02_create_dataset/#generate-datasets","title":"Generate Datasets","text":""},{"location":"03_howto/02_create_dataset/#add-files","title":"Add files","text":""},{"location":"03_howto/02_create_dataset/#update-datasets","title":"Update datasets","text":""},{"location":"03_howto/03_generate_workflow/","title":"Generate and update workflows","text":""},{"location":"03_howto/04_update_object/","title":"Update OmniObject","text":""},{"location":"03_howto/05_filter/","title":"Filter Inputs and Parameter to be excluded","text":"<p>Some methods are not applicable for all input datasets or the entire parameter range. For example, perhaps the parameter range to use depends on features of the input dataset. <code>omnibenchmark</code> allows filtering (i.e., to filter out or remove) at different levels to specify input data bundles to exclude, parameter limits and input-parameter combinations to ignore. See below for different types of filters with examples.</p> <p>Note</p> <p>In general <code>omnibenchmark</code> allows detailed specification of the accepted inputs and parameter to include, e.g., through keywords or explicit definition. Especially keyword-based specification is limited by its extent to generalize for new (not yet included) inputs/parameter. Filtering inputs and outputs to exclude allows automatic and programmatic specification to extend existing generalizations.     </p>"},{"location":"03_howto/05_filter/#filter-explicit-input-data-bundles","title":"Filter explicit input data bundles","text":"<p>While input data bundles with a certain keyword should typically fullfill all requirements of downstream modules, there can be module-specifc requirements, which are violated by single upstream data bundles. A possible solution is to explicitly exclude certain data bundles by name as inputs. These data bundles will automatically be ignored upon data bundle import and input - output file mapping. Input filtering can be specified within the <code>config.yaml</code> or by using attributes of the OmniObject in <code>run_workflow.py</code>:</p> config.yamlrun_workflow.py <pre><code>---\ndata:\n...\nscript: \"src/run_method_XY.R\"\nbenchmark_name: \"omniexample\"\ninputs:\nkeywords: [\"dataset_omniexample\"]\nfiles: [\"counts\", \"meta_file\"]\nprefix:\ncounts: \"_counts\"\nmeta_file: \"_meta\"\nfilter_names: [\"dataset1\", \"dataset2\"] # (1)!\noutputs:\n...\n</code></pre> <ol> <li>Names must correspond to data bundle names (also called slug in renku terminology). A single name can be provided as string and multiple names as list of strings.</li> </ol> <pre><code>import omnibenchmark as omni\n# Generate object from yaml file\nomni_obj = omni.utils.get_omni_object_from_yaml(\"config.yaml\")\n# Explicitly ignore input data bundles by name\nomni_obj.inputs.filter_names = [\"dataset1\", 'dataset2']\n# Update object\nomni_obj.update_object() # (1)!\nrenku_save()\n</code></pre> <ol> <li>Add filter attributes before updating the object to prevent data bundles you want to ignore from being imported. </li> </ol> <p>Note</p> <p>Consider using different input data bundle keywords, if the number of data bundles to explicitly exclude is large or multiple downstream modules have the same restrictions.</p>"},{"location":"03_howto/05_filter/#filter-parameter","title":"Filter parameter","text":"<p>Parameter ranges can be explicitly specified within the <code>config.yaml</code> file. If the global parameter space (i.e., a data bundle with the benchmark-specific parameter space) is used, specific limits or values to exclude can be specified within the <code>config.yaml</code> file or as attributes of the OmniObject in <code>run_workflow.py</code>:</p>"},{"location":"03_howto/05_filter/#parameter-limits","title":"Parameter limits","text":"<p>Limits can only be used to specify ranges of parameter with numeric values. Limits for each parameter are specified separately.  The keys <code>upper</code> and <code>lower</code> are fixed terms.</p> config.yamlrun_workflow.py <pre><code>---\ndata:\n...\nscript: \"src/run_method_XY.R\"\nbenchmark_name: \"omniexample\"\ninputs:\n...\noutputs:\n...\nparameter:\nnames: [\"dims\", \"mode\"]\nkeywords: [\"parameter_omniexample\"]\nfilter: # (1)!\ndims:\nupper: 100\nlower: 10\n</code></pre> <ol> <li>Limits can only be specified for parameter with numeric values. The keys <code>upper</code> and <code>lower</code> are fixed and required.</li> </ol> <pre><code>import omnibenchmark as omni\n# Generate object from yaml file\nomni_obj = omni.utils.get_omni_object_from_yaml(\"config.yaml\")\n# Set parameter limits # (1)!\nomni_obj.parameter.filter = {\n\"dims\": {\n\"lower\": 10, \n\"upper\": 100\n}\n}\n# Update object\nomni_obj.update_object() # (2)!\nrenku_save()\n</code></pre> <ol> <li>Limits can only be specified for parameter with numeric values. The keys <code>upper</code> and <code>lower</code> are fixed and required. </li> <li>Add filter attributes before updating the object to apply filter to the generated outputs. </li> </ol>"},{"location":"03_howto/05_filter/#parameter-values-to-exclude","title":"Parameter values to exclude","text":"<p>Specific values can be ignored/excluded for all inputs. Parameter values to exclude are specified explicitly for each parameter.  The key <code>exclude</code> is a fixed term.</p> config.yamlrun_workflow.py <pre><code>---\ndata:\n...\nscript: \"src/run_method_XY.R\"\nbenchmark_name: \"omniexample\"\ninputs:\n...\noutputs:\n...\nparameter:\nnames: [\"dims\", \"mode\"]\nkeywords: [\"parameter_omniexample\"]\nfilter: # (1)!\nmode:\nexclude: [\"auto\", \"solo\"]\n</code></pre> <ol> <li>Values to exclude can be specified for parameter with numeric and character values. The key <code>exclude</code> is fixed and required.</li> </ol> <pre><code>import omnibenchmark as omni\n# Generate object from yaml file\nomni_obj = omni.utils.get_omni_object_from_yaml(\"config.yaml\")\n# Set parameter to exclude # (1)!\nomni_obj.parameter.filter = {\n\"mode\": {\n\"exclude\": [\"auto\", \"mode\"]\n}\n}\n# Update object\nomni_obj.update_object() # (2)!\nrenku_save()\n</code></pre> <ol> <li>Values to exclude can be specified for parameter with numeric and character values. The key <code>exclude</code> is fixed and required. </li> <li>Add filter attributes before updating the object to apply filter to the generated outputs. </li> </ol>"},{"location":"03_howto/05_filter/#filter-explicit-input-parameter-combinations","title":"Filter explicit input-parameter combinations","text":"<p>Explicit input parameter combinations can be filtered by providing a <code>.json</code> file explicitly listing the input - parameter combinations (mappings) to be ignored and not used for output generation. </p> filter_combinations.json<pre><code>[\n{\n\"input_files\": {\n\"in_file1\": \"data/dataset1/dataset1_in_file1.mtx.gz\",\n\"in_file2\": \"data/dataset1/dataset1_in_file2.json\",\n},\n\"parameter\": {\n\"dims\": 10,\n\"mode\": \"auto\"\n}\n}\n]\n</code></pre> <p>The <code>.json</code> file with explicit mappings to ignore can be specified in the <code>config.yaml</code> file or as attributes of the OmniObject in <code>run_workflow.py</code>:</p> config.yamlrun_workflow.py <pre><code>---\ndata:\n...\nscript: \"src/run_method_XY.R\"\nbenchmark_name: \"omniexample\"\ninputs:\n...\noutputs:\nfiles:\nmethod_res: end: \".json\"\nfilter_json: \"path/to/filter_combinations.json\"\nparameter:\n...\n</code></pre> <pre><code>import omnibenchmark as omni\n# Generate object from yaml file\nomni_obj = omni.utils.get_omni_object_from_yaml(\"config.yaml\")\n# Path to filter combinations.json \nomni_obj.outputs.filter_json = \"src/filter_comb.json\"\n# Update object\nomni_obj.update_object() # (1)!\nrenku_save()\n</code></pre> <ol> <li>Add filter attributes before updating the object to apply filter to the generated outputs. </li> </ol>"},{"location":"03_howto/05_filter/#programmatic-filter","title":"Programmatic filter","text":"<p>If possible any filter should be specified programmatically to allow generalization for new (not yet known) inputs or parameter space extension. We can use the above-mentioned <code>.json</code> file from explicitly filtering input-parameter combinations to automatically filter input data bundles, parameter values or input-parameter combinations.</p> <p>Note</p> <p>Programmatic filtering can not be done on the <code>config.yaml</code> level, but needs to be implemented within <code>run_workflow.py</code>.</p> <ol> <li> <p>Define a filter function describing your filter in a general way.</p> Example <p>The following example uses the <code>omni_obj</code> as input argument, gets the true dimensionality of the data from the input file with the key <code>meta</code> and selects values from the <code>dims</code> parameter that differ more than 3 from the true dimensionality for filtering.</p> filter_function.py<pre><code>import json\nimport pandas as pd\ndef get_param_filter_by_ground_truth(omni_obj):\nfilter_list = []\n# Loop through all input file groups\nfor infile_mapping in omni_obj.inputs.input_files.values():\n# Load meta data from object\nwith open(infile_mapping[\"meta_file\"]) as f:  \nmeta = json.load(f)\n# Get true dim\ndf = pd.DataFrame.from_dict(meta)\nk = meta.get(\"true_dim\")\n# Get parameter combinations to filter\nparam_comb = [\n{param_nam: param_val for param_nam, param_val in param_map.items()}\nfor param_map in omni_obj.parameter.combinations\nif param_map[\"k\"] &lt; k - 3 or param_map[\"k\"] &gt; k + 3\n]\n# Generate filter items\nfilter_list_dat = [\n{\"input_files\": infile_mapping, \"parameter\": param_com}\nfor param_com in param_comb\n]\n# Add to filter_list\nfilter_list = filter_list + filter_list_dat\nreturn filter_list\n</code></pre> </li> <li> <p>Store filtered combinations as <code>.json</code> and assign this as output filter <code>.json</code> file</p> run_workflow.py<pre><code>from omnibenchmark.utils.build_omni_object import get_omni_object_from_yaml\nfrom omnibenchmark.renku_commands.general import renku_save\nimport json\nimport filter_function # (1)!\n## Load config\nomni_obj = get_omni_object_from_yaml('src/config.yaml')\n## Update object and download input datasets\nomni_obj.update_object()\nrenku_save()\n## Generate json with filter combinations\nfilter_comb = filter_function.get_param_filter_by_ground_truth(omni_obj) # (2)!\nwith open(\"filter_comb.json\", \"w\") as fp:\njson.dump(filter_comb, fp, indent=3)\nrenku_save()\n</code></pre> <ol> <li>Import filter function from Step 1</li> <li>See example above (adapt function name and input) </li> </ol> </li> <li> <p>Update outputs and command to apply filter</p> run_workflow.py<pre><code>## update outputs and commands\nomni_obj.outputs.filter_json = \"src/filter_comb.json\" # (1)!\nomni_obj.outputs.update_outputs()\nomni_obj.command.outputs = omni_obj.outputs\nomni_obj.command.update_command()\n## Run workflow\nomni_obj.run_renku()\nrenku_save()\n</code></pre> <ol> <li>Updating outputs and command only (instead of the entire object) to prevent input import/update.</li> </ol> </li> </ol>"},{"location":"04_bugs/","title":"Index","text":""},{"location":"04_bugs/#common-issues","title":"Common issues","text":"<p>List of common issues and their solutions. </p>"},{"location":"04_bugs/01_templates/","title":"Templates project creation","text":""},{"location":"04_bugs/01_templates/#issues-related-to-templates-and-project-creation","title":"Issues related to templates and project creation","text":"<p>More to come...</p>"},{"location":"04_bugs/02_python/","title":"Issues related to Python CLIs","text":""},{"location":"04_bugs/02_python/#-dirtyrepository-the-repository-is-dirty-please-use-the-git-command-to-clean-it","title":"- <code>DirtyRepository: The repository is dirty. Please use the \"git\" command to clean it.</code>","text":"<p>Context: a renku/ omnibenchmark related command is run and returns an exit status with the above message.</p> <p>Explanation: Renku (and Omnibenchmark) commands can only be run when there is no unsaved work.</p> <p>Solution: run <code>renku_save()</code> (Python) or <code>renku save</code> (Bash) and rerun your command.</p>"},{"location":"04_bugs/02_python/#issues-related-to-omni_objupdate_objectalltrue","title":"Issues related to <code>omni_obj.update_object(all=True)</code>","text":"<p>Context: an error is raised when running the above command and no input data is imported. </p> <p>Explanation: there might be different reasons; the orchestrator is not set up, there is no input data,...</p> <p>Solutions: depending on the problem, try the followings: </p> <ul> <li>check that an orchestrator is set up for your Omnibenchmark: https://github.com/omnibenchmark/omni_essentials/blob/main/general/benchmark_categories.json</li> </ul>"},{"location":"04_bugs/02_python/#-omni_objupdate_result_dataset-parametererror-invalid-parameter-value-these-datasets-dont-exist-or-omni_objcreate_dataset-dataset-zhengmix4eq-filterhvg-pinned-already-taken-please-use-a-different-name","title":"- <code>omni_obj.update_result_dataset()</code> &gt; <code>ParameterError: Invalid parameter value - These datasets don't exist:</code> OR <code>omni_obj.create_dataset()</code> &gt; <code>Dataset zhengmix4eq-filterhvg-pinned already taken. Please use a different name.</code>","text":"<p>Context: Error or warning message when creating a dataset in an Omnibenchmark project</p> <p>Explanation: the name is already taken or has conflicting dataset name. Typically, a subset of another name that our name matching algorithms could mix. Example: \"dataset\" vs \"dataset-updated\". </p> <p>Solution: change the <code>name:</code> in your <code>config.yml</code> to resolve the conflict; try to avoid spaces and <code>-</code> and rather use <code>_</code> and more descriptive names.</p>"},{"location":"04_bugs/02_python/#-giterror-cannot-commit-changes","title":"- <code>GitError: Cannot commit changes</code>","text":"<p>Context: error when running a workflow. </p> <p>Explanation: It is possible that some files were generated in your <code>data</code> folder, where the output of your workflow will be moved. For reproducibility and tracing purpose, Renku doesn't allow files generation into your output <code>data</code> folder, if they were not generated by a Renku workflow. </p> <p>Solution: remove the problematic files from your <code>data</code> folder and run again your workflow command. </p>"},{"location":"04_bugs/02_python/#-filenotfounderror-errno-2-no-such-file-or-directory-optcondalibpython39site-packagesomnivalidatorschemas","title":"- <code>FileNotFoundError: [Errno 2] No such file or directory: '/opt/conda/lib/python3.9/site-packages/omniValidator/schemas/...'</code>","text":"<p>Explanation: you are using an older version of <code>omniValidator</code>. </p> <p>Solution: upgrade <code>omniValidator</code> to &gt; 0.0.19</p>"},{"location":"04_bugs/02_python/#-interpretererror-interpreter-could-not-be-identified-from-extention-please-specify-command-explicitly","title":"- <code>InterpreterError: Interpreter could not be identified from extention . Please specify command explicitly.</code>","text":"<p>Context: error when calling <code>get_omni_object_from_yaml('src/config.yaml')</code></p> <p>Explanation: your main script likely lacks a complete name (e.g. <code>.R</code>, <code>.py</code>).</p> <p>Solution: give an explicit name to your script (e.g. <code>run_method.R</code>) and to your <code>config.yaml</code> file (<code>script:</code> field). </p>"},{"location":"04_bugs/02_python/#-one-or-several-of-my-data-files-are-only-3-lines-long-but-the-initial-content-is-not-here","title":"- One or several of my data files are only 3 lines long but the initial content is not here","text":"<p>Context: one or several of your data files contain a structure of type: </p> <pre><code>version https://git-lfs.github.com/spec/v1\noid sha256:XXXXXXXXXXXXXXXXXXXXXXXXXXXX\nsize XXXX\n</code></pre> <p>Explanation: one or several of your data files have been stored on Git LFS. By default, all large files are pushed there to save memory. More information on the renku documentation.</p> <p>Solution: on an interactive session, check the <code>fetch automatically LFS data</code> option. In a terminal, you can also fetch the data with <code>git lfs pull</code>. </p>"},{"location":"04_bugs/03_renku_interface/","title":"Renku interface","text":""},{"location":"04_bugs/03_renku_interface/#issues-related-to-renku-website-and-interface","title":"Issues related to Renku website and interface","text":"<p>More to come...</p>"},{"location":"05_events/","title":"Index","text":""},{"location":"05_events/#past-and-present-events","title":"Past and present events","text":"<ul> <li> <p>Robinsons hackathons </p> </li> <li> <p>[BC]2 tutorial half-day (11.09.23)</p> </li> </ul>"},{"location":"05_events/robinsons_hackathons/","title":"Robinson Hackathons","text":""},{"location":"05_events/robinsons_hackathons/#omnibenchmark-hackathons","title":"Omnibenchmark hackathons","text":""},{"location":"05_events/robinsons_hackathons/#september-2022","title":"September 2022","text":"<ul> <li>Working document</li> </ul>"},{"location":"05_events/BC2/","title":"[BC]2 Half-day tutorial - September 11th 2023","text":""},{"location":"05_events/BC2/#resources","title":"Resources","text":"<ul> <li>Working document</li> </ul>"},{"location":"05_events/BC2/#sections","title":"Sections","text":"<ul> <li> <p>Step-by-step guide</p> </li> <li> <p>Presentations</p> </li> <li> <p>Hands-on sessions</p> </li> </ul>"},{"location":"05_events/BC2/#overview","title":"Overview","text":"<p>Intermediate level \u2013\u2013  Method benchmarking  \u2013\u2013  benchmarks  \u2013\u2013  omics datasets</p> <p>The rise of large-scale omics datasets has led to a growing number of methods to model and interpret them, making it hard to identify performant computational methods to use in discovery research. Method benchmarking is critical to dissect important steps of an analysis pipeline, formally assess performance across common situations and edge cases, and ultimately guide users. While some form of comparison is standard practice in method development, current approaches have several limitations (Sonrel et al. 2022). One main issue is that benchmarks are not easily extensible and with the constant emergence of new approaches, this often leads to rapidly outdated or even contradicting and irreproducible conclusions. We believe that if benchmarks were organized in a systematic way and conducted at a higher standard, they will have a considerably greater impact in guiding best practice in employing computational methods for discovery research.</p> <p>To address these issues, we created Omnibenchmark, a modular and extensible framework based on the free open-source platform renku. The framework connects data, method, and metric modules via a knowledge graph, which tracks relevant metadata (e.g. software environment, parameters and commands). Results can be viewed in a dashboard or openly accessed, and new modules can be added easily with pre-configured templates. To facilitate community contributions for adding new reference datasets, evaluation metrics or computational methods, we maintain a series of pre-configured templates. Each element of Omnibenchmark is packaged with all dependencies and can be inspected, re-used, modified, and integrated onto other platforms in compliance with the FAIR principles. Learning objectives</p> <p>At the end of the tutorial, participants should be able to:</p> <ul> <li> <p>understand components of the Omnibenchmark framework</p> </li> <li> <p>submit new reference datasets, methods or metrics modules</p> </li> <li> <p>visualize and re-use output results and performance summaries</p> </li> </ul> Time Activity 09:00 \u2013 09:30 Introduction to Omnibenchmark framework 09:30 \u2013 10:30 Group creation &amp; first hands-on session following our step-by-step guide 10:30 \u2013 10:45 Coffee break 10:45 - 11:15 Technical aspects of Omnibenchmark module creation 11:15 \u2013 12:00 Second hands-on session 12:00 \u2013 12:15 Q&amp;A and Closing remarks 12:15 \u2013 13:00 Lunch -  Join us for lunch, even if you only attend the morning workshop!"},{"location":"05_events/BC2/#audience-and-requirements","title":"Audience and requirements","text":"<p>Maximum number of participants: 20</p> <p>This tutorial is addressed to computational-minded researchers who are tasked with benchmarking methods that they are familiar with (or that they developed) as well as for benchmarkers that would like to port their work to our system.</p> <p>Participants are expected to have : - Intermediate knowledge of R or Python - Basic knowledge of UNIX and version control systems (Git) - Personal laptop with Wifi connection - Code for method(s) and/ or benchmark(s)- (optional)</p>"},{"location":"05_events/BC2/#organizers","title":"Organizers","text":"<ul> <li> <p>Main presenters: Almut Lu\u0308tge and Anthony Sonrel, PhD Students and co-developers of Omnibenchmark, Statistical Bioinformatics Group at the University of Zurich, Switzerland</p> </li> <li> <p>Technical collaborators and knowledge exchange: Dr. Izaskun Mallona, Statistical Bioinformatics Group at the University of Zurich (co-developer of Omnibenchmark); Dr. Charlotte Soneson, Research Associate, Computational Biology Platform, FMI and SIB Swiss Institute of Bioinformatics, Switzerland</p> </li> <li> <p>Introductions and support: Mark D. Robinson, Associate Professor, University of Zurich and SIB Group Leader, Switzerland</p> </li> </ul>"},{"location":"05_events/BC2/hands-on_sessions/","title":"Hands-on sessions - omniclustering","text":""},{"location":"05_events/BC2/hands-on_sessions/#resources","title":"Resources","text":"<p>Choose a module to add to the Omniclustering benchmark. Alternatively, you can also work on the Iris example from the step-by-step guide. </p> <ul> <li> <p>Downloadable single-cell datasets</p> </li> <li> <p>List of wrappers of methods that perform single-cell clustering</p> </li> </ul>"},{"location":"05_events/BC2/hands-on_sessions/#projects-assignments","title":"Projects assignments","text":"<p>To avoid overlaps, please specify which projects you are working on: </p> <p>https://docs.google.com/spreadsheets/d/1nnPSALl6up0-yjXiWLPe0R7gSUyveozUQdVmZLu2fno/edit?usp=sharing</p>"},{"location":"05_events/BC2/hands-on_sessions/#project-creation","title":"Project creation","text":"<p>See the Create a module - Clustering Omnibenchmark links to prefilled projects. </p> <p>Warning</p> <p>Please use an explicit name for easier recognition and also to avoid names overlap. E.g. <code>method_[method name]_clustering</code>. </p>"},{"location":"05_events/BC2/hands-on_sessions/#project-set-up","title":"Project set up","text":"<p>See the dedicated guides for Data modules, Methods modules or Metric modules.</p>"},{"location":"05_events/BC2/hands-on_sessions/#project-population","title":"Project population","text":"<p>You can find information about the required metadata (keywords, inputs, outputs, etc) in two ways: </p>"},{"location":"05_events/BC2/hands-on_sessions/#1-copy-the-structure-of-existing-modules","title":"1. Copy the structure of existing modules","text":"<p>Warning</p> <p>Please do not fork those to avoid transferring unintended metadata from the original project; rather, copy the module structure manually</p> <ul> <li> <p>Example Dataset</p> </li> <li> <p>Example Method</p> </li> <li> <p>Example Metric</p> </li> </ul>"},{"location":"05_events/BC2/hands-on_sessions/#2-use-the-omnivalidator-inside-your-session","title":"2. Use the <code>omniValidator</code> inside your session","text":"<p>The requirements for a given omnibenchmark and keyword can be displayed by running the following lines: </p> <pre><code>import omniValidator as ov\nov.display_requirements(\n    benchmark='BENCHMARK_NAME', \n    keyword='STEP_KEYWORD')\n</code></pre> <p>where  <code>BENCHMARK_NAME</code> is the name of the benchmark (<code>omniclustering</code>) and <code>STEP_KEYWORD</code> is the keyword associated to the type of module you are working on (<code>omniclustering_dataset</code>, <code>omniclustering_method</code> or <code>omniclustering_metric</code>).</p> <p>Info</p> <p><code>omniValidator</code> is ported with all projects and doesn't need to be installed.</p>"},{"location":"05_events/BC2/hands-on_sessions/#output-omnibenchmark-status","title":"Output / Omnibenchmark status","text":"<p>An overview of the Omnibenchmark components is available on the Omnibenchmark webpage</p> <p>The output of the benchmark is available on our shiny app server.</p>"},{"location":"05_events/BC2/presentations/","title":"Presentation","text":"<ul> <li> <p>Intro (Anthony Sonrel)</p> </li> <li> <p>Technical details (Anthony Sonrel)</p> </li> <li> <p>Technical details (Almut L\u00fctge)</p> </li> <li> <p>Intro to clustering Omnibenchmark (Charlotte Soneson)</p> </li> </ul>"},{"location":"05_events/BC2/step_by_step/","title":"Step-by-step guide","text":"<p>This short guide will show you how to add a method's wrapper to Omnibenchmark. We will add a classification method to our example Iris omnibenchmark (a simple benchmark using the Iris dataset). </p> <p>Please add your username and mail in this google sheet for the rest of the turorial.  </p>"},{"location":"05_events/BC2/step_by_step/#1-create-a-new-project","title":"1. Create a new project","text":"<p>Each of an Omnibenchmark is basically a Renku project (a Gitlab project with many tweaks). To add a new method to the Iris Omnibenchmark, we will create a new Renku project to host our code. Luckily, you don't have to code a whole new Renku project from scratch! You will see how to add pre-filled project to any Omnibenchmark: </p> <p>Note</p> <p>Congrats, you just created your first project on Renku using our dedicated Omnibenchmark templates (pre-filled projects). The projects also come with their own set of instructions (in the README) but you can ignore these for now. The next sessions will cover them. </p>"},{"location":"05_events/BC2/step_by_step/#2-set-up-a-working-environment","title":"2. Set up a working environment","text":"<p>We have created a new project to host our code. One easy way to work on a project is to start an interactive session to work interactively on our project. Let's see how to set this up: </p>"},{"location":"05_events/BC2/step_by_step/#3-add-some-code-to-your-project","title":"3. Add some code to your project","text":"<p>We can now modify, run and save our work from an interactive session. Let's see how to add code to work with the Iris dataset; </p>"},{"location":"05_events/BC2/step_by_step/#a-set-up-the-metadata-for-the-project","title":"A. Set up the metadata for the project","text":"<p>Open <code>src/config.yaml</code>. This file tells Omnibenchmark how to run the project and the metadata associated to it. Modify the file as follows (highlighted lines): </p> src/config.yaml <pre><code>---\ndata:\nname: \"iris-method-bc2-[YOUR_NAME]\"\ntitle: \"iris method example BC2\"\ndescription: \"\"\nkeywords: [\"iris_method_bc2\"]\nscript: \"src/iris-method-bc2-anthonysonrel1ecd.R\"\nbenchmark_name: \"iris_example\"\ninputs:\nkeywords: [\"iris_dataset\"] ## Keyword(s) for the input dataset to import. \nfiles: [\"input\"]  ## Input file type(s).\nprefix:\ninput: \"_dataset\"\noutputs:\ntemplate: \"data/${name}/${name}_${unique_values}_${out_name}.${out_end}\" ## Automatic. To change if you want specific output names. \nfiles:\n## File patterns to use for automatic detection. \nmethod_result1: end: \"rds\"\nparameter:\n## Section to describe the parameter dataset, values and filter. \nkeywords: [\"iris_parameters\"] ## Keyword(s) of the parameters project(s) to import.\nnames: [\"rseed\"] ## Name(s) of the parameter(s) to use from the parameter project that is imported.\n</code></pre> <p>Info</p> <p>The <code>src/config.py</code> is ported with all omnibenchmark projects. It is always pre-filled for you and is usually the first file that you have to modify.</p>"},{"location":"05_events/BC2/step_by_step/#b-add-code-to-the-project","title":"B. Add code to the project","text":"<p>Open your R script: <code>src/iris-method-bc2-[...].R</code>. Copy-paste the following code in your script (we'll go through it together); </p> src/iris-method-bc2[...].R <pre><code># Load package\nlibrary(optparse)\n# Get list with command line arguments by name\noption_list = list(\nmake_option(c(\"--input\"), type=\"character\", default=NULL, help=\"Description of the argument\", metavar=\"character\"), make_option(c(\"--rseed\"), type=\"character\", default=NULL, help=\"Description of the argument\", metavar=\"character\"), make_option(c(\"--method_result1\"), type=\"character\", default=NULL, help=\"Description of the argument\", metavar=\"character\")\n); opt_parser = OptionParser(option_list=option_list);\nopt = parse_args(opt_parser);\n# Call the arguments\ninput &lt;- opt$input\nrseed &lt;- opt$rseed\nmethod_result1 &lt;- opt$method_result1\n# Call the method\nlibrary(caret)\ndat &lt;- read.csv(input)\ndat$Species &lt;- as.factor(dat$Species)\n# Split\nvalidation_index &lt;- createDataPartition(dat$Species, p=.5, list=FALSE)\nvalidation &lt;- dat[-validation_index,]\ndat_train &lt;- dat[validation_index,]\nset.seed(rseed)\ncontrol &lt;- trainControl(method=\"cv\", number=10)\nmetric &lt;- \"Accuracy\"\nfit.lda &lt;- train(Species~., data=dat_train, method=\"lda\", metric=metric, trControl=control)\nprint(fit.lda)\nsaveRDS(object = fit.lda, file = method_result1)\n</code></pre>"},{"location":"05_events/BC2/step_by_step/#c-run-the-workflow","title":"C. Run the workflow","text":"<p>We have specified how the project should run and the code associated to it. </p> <p>We can now run the workflow by running the <code>src/run_workflow.py</code>. </p> <p>Open a new Python console from the base directory. We will go through it together.</p> <p>Info</p> <p>The <code>src/run_workflow.py</code> is ported with all omnibenchmark projects. Its only purpose is to run your code and doesn't need to be modified. </p>"}]}